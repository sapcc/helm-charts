owner-info:
  support-group: observability
  service: elastiflow
  maintainers:
    - "Nathan Oyler"
    - "Ivo Gosemann"
  helm-chart-url: "https://github.com/sapcc/helm-charts/tree/master/system/elastiflow"

# Deploy Prometheus alerts.
alerts:
  enabled: false
  # Name of the Prometheus to which the alerts should be assigned to.
  prometheus:
    - name: infra-frontend
      type: prometheus
    - name: scaleout
      type: thanos-ruler

curator:
  enabled: false
  retention: "92"
  data_fw_retention: "3"
  env:
    DATA_RETENTION: "$(RETENTION)"
    ELASTIC_CREDS: "$(USERNAME):$(PASSWORD)"
  envFromSecrets:
    RETENTION:
      from:
        secret: curator-secrets
        key: retention
    USERNAME:
      from:
        secret: curator-secrets
        key: username
    PASSWORD:
      from:
        secret: curator-secrets
        key: password
  nodeSelector:
    ccloud.sap.com/nodepool: elastiflow
  tolerations:
    - key: "dedicated"
      operator: Equal
      value: "elastiflow"
      effect: "NoSchedule"
  configMaps:
    config_yml: |-
      ---
      client:
        hosts:
          - elastiflow-data
        port: 9200
        http_auth: ${ELASTIC_CREDS}
      logging:
        loglevel: INFO
    # Delete indices older than 3 months
    action_file_yml: |-
      ---
      actions:
        1:
          action: delete_indices
          description: "Clean up ES by deleting old indices"
          options:
            timeout_override:
            continue_if_exception: False
            disable_action: False
            ignore_empty_list: True
            allow_ilm_indices: True
          filters:
          - filtertype: pattern
            kind: prefix
            value: .kibana
            exclude: True
          - filtertype: pattern
            kind: prefix
            value: .task
            exclude: True
          - filtertype: age
            source: name
            direction: older
            timestring: '%Y.%m.%d'
            unit: days
            unit_count: ${DATA_RETENTION}
            field:
            stats_result:
            epoch:
            exclude: False
        2:
          action: delete_indices
          description: "Clean up ES by deleting old fw drop indices"
          options:
            timeout_override:
            continue_if_exception: False
            disable_action: False
            ignore_empty_list: True
            allow_ilm_indices: True
          filters:
          - filtertype: pattern
            kind: regex
            value: 'elastiflow-4.0.1-fw-.*'
          - filtertype: age
            source: name
            direction: older
            timestring: '%Y.%m.%d'
            unit: days
            unit_count: 3
            field:
            stats_result:
            epoch:
            exclude: False

# master node configs
elasticsearch:
  enabled: false
  annotations:
    kubernetes.io/tls-acme: true
    disco: true
  exporter:
    enabled: true
    prometheus: infra-frontend

  roles:
    master: "true"
    ingest: "false"
    data: "false"
  replicas: 3
  esJavaOpts: "-Dlog4j2.formatMsgNoLookups=true -Xmx2G -Xms2G"
  esConfig:
    elasticsearch.yml: |
      indices.query.bool.max_clause_count: 8192
      search.max_buckets: 250000
      xpack.security.enabled: false

    log4j2.properties: |
      logger.rorignore.name = tech.beshu.ror.es.request.queries
      logger.rorignore.level = error
      logger.rorignore.additivity = false

  volumeClaimTemplate:
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 1G

  extraInitContainers: |
    - name: configure-ownership
      securityContext:
        runAsUser: 0
      image: "{{ required ".Values.global.registry variable missing" .Values.global.registry }}/{{ .Values.global.elastic.image}}:{{ .Values.global.elastic.tag}}"
      imagePullPolicy: "{{ .Values.imagePullPolicy }}"
      command: ["sh", "-c", "mkdir -p /usr/share/elasticsearch/data && chown -R 1000:1000 /usr/share/elasticsearch/data"]
      volumeMounts:
        - name: "{{ template "elasticsearch.uname" . }}"
          mountPath: /usr/share/elasticsearch/data

elasticsearch-data:
  enabled: false
  nodeGroup: data
  masterService: elastiflow-master
  roles:
    master: "false"
    ingest: "true"
    data: "true"
  replicas: 2
  esJavaOpts: "-Dlog4j2.formatMsgNoLookups=true -Xmx10G -Xms10G"
  resources:
    requests:
      cpu: "4000m"
      memory: "14Gi"
    limits:
      cpu: "6000m"
      memory: "14Gi"
  esConfig:
    elasticsearch.yml: |
      indices.query.bool.max_clause_count: 8192
      search.max_buckets: 250000
      xpack.security.enabled: false

    log4j2.properties: |
      logger.rorignore.name = tech.beshu.ror.es.request.queries
      logger.rorignore.level = error
      logger.rorignore.additivity = false

  volumeClaimTemplate:
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 500G

  extraInitContainers: |
    - name: configure-ownership
      securityContext:
        runAsUser: 0
      image: "{{ required ".Values.global.registry variable missing" .Values.global.registry }}/{{ .Values.global.elastic.image}}:{{ .Values.global.elastic.tag}}"
      imagePullPolicy: "{{ .Values.imagePullPolicy }}"
      command: ["sh", "-c", "mkdir -p /usr/share/elasticsearch/data && chown -R 1000:1000 /usr/share/elasticsearch/data"]
      volumeMounts:
        - name: "{{ template "elasticsearch.uname" . }}"
          mountPath: /usr/share/elasticsearch/data
  exporter:
    enabled: true
    prometheus: infra-frontend

logstash:
  enabled: false
  image: "elastiflow-logstash"
  imageTag: "DEFINED-IN-PIPELINE"
  logstashJavaOpts: "-Xmx8G -Xms8G"

  persistence:
    enabled: false

  resources:
    requests:
      cpu: 4000m
      memory: 8G
    limits:
      cpu: 4000m
      memory: 10G

  elastic:
    shards: 3
    replicas: 1
  extraEnvs:
    - name: ELASTIFLOW_DNS_HIT_CACHE_TTL
      value: "604800"
    - name: ELASTIFLOW_RESOLVE_IP2HOST
      value: "exporters"
  jdbc:
    enabled: false
    configMap: "jdbc-filter"
    configFile: "20_filter_95_enrich_ips.conf"
    schedule: "0 13 * * *"
  fw:
    enabled: true
    configMap: "fw-output"
    configFile: "30_output_10_single.logstash.conf"
  service:
    # type: NodePort
    # externalTrafficPolicy: Local
    clusterIP: None
    ports:
      - name: beat-tcp
        port: 5044
        protocol: TCP
        targetPort: 5044
      # - name: netflow-udp
      #   port: 2055
      #   targetPort: 2055
      #   nodePort: 30055
      #   protocol: UDP
      # - name: sflow-udp
      #   port: 6343
      #   targetPort: 6343
      #   nodePort: 30343
      #   protocol: UDP
      # - name: ipfix-udp
      #   port: 4739
      #   targetPort: 4739
      #   nodePort: 30739
      #   protocol: UDP
      # - name: ipfix-tcp
      #   port: 4739
      #   targetPort: 4739
      #   nodePort: 30740
      #   protocol: TCP
      # - name: http
      #   port: 9600
      #   targetPort: 9600
      #   protocol: TCP
  alerts:
    enabled: false
    prometheus: infra-collector

  readinessProbe:
    # use beats port instead of http API to better check elastiflow specific status
    httpGet: null
    tcpSocket:
      port: 5044
    initialDelaySeconds: 60
    periodSeconds: 30
    failureThreshold: 20
    successThreshold: 1

  livenessProbe:
    httpGet: null
    tcpSocket:
      port: 5044
    initialDelaySeconds: 600
    periodSeconds: 20
    failureThreshold: 5
    successThreshold: 1

logstashExporter:
  name: elastiflow
  enabled: false
  image:
    repo: bonniernews/logstash_exporter
    tag: v0.1.2
  ls:
    uri: http://elastiflow-logstash:9600
  listen_port: 9198
  targets: infra-collector

elastiflow:
  externalIPs:
  sflow:
    enabled: false

filebeat:
  enabled: false
  image: "elastiflow-filebeat"
  imageTag: "20220331081029"
  resources:
    requests:
      cpu: "500m"
      memory: "1G"
    limits:
      cpu: "2000m"
      memory: "16G"

global:
  redis:
    enabled: false
    host: "elastiflow-redis"
    logstash_imageTag: "202110081140"
    filebeat_imageTag: "202110071400"
  elastic:
    image: elastiflow-elastic
    tag: <defined-in-pipeline>

redis:
  enabled: false
  resources:
    limits:
      memory: 8G
      cpu: 1000m
    requests:
      memory: 8G
      cpu: 500m
  metrics:
    enabled: true
    prometheus: infra-collector

kibana:
  enabled: false
  image: "kibana/kibana"
  elasticsearchHosts: "http://elastiflow-master:9200"
  # resources:
  #   requests:
  #     memory: 4Gi
  #   limits:
  #     memory: 5Gi
