curator:
  enabled: false
  retention: "92"
  env:
    DATA_RETENTION: "$(RETENTION)"
    ELASTIC_CREDS: "$(USERNAME):$(PASSWORD)"
  envFromSecrets:
    RETENTION:
      from:
        secret: curator-secrets
        key: retention
    USERNAME:
      from:
        secret: curator-secrets
        key: username
    PASSWORD:
      from:
        secret: curator-secrets
        key: password
  nodeSelector:
    ccloud.sap.com/nodepool: elastiflow
  tolerations:
  - key: "dedicated"
    operator: Equal
    value: "elastiflow"
    effect: "NoSchedule"
  configMaps:
    config_yml: |-
      ---
      client:
        hosts:
          - elastiflow-data
        port: 9200
        http_auth: ${ELASTIC_CREDS}
      logging:
        loglevel: INFO
    # Delete indices older than 3 months
    action_file_yml: |-
      ---
      actions:
        1:
          action: delete_indices
          description: "Clean up ES by deleting old indices"
          options:
            timeout_override:
            continue_if_exception: False
            disable_action: False
            ignore_empty_list: True
            allow_ilm_indices: True
          filters:
          - filtertype: pattern
            kind: prefix
            value: .kibana
            exclude: True
          - filtertype: pattern
            kind: prefix
            value: .task
            exclude: True
          - filtertype: age
            source: name
            direction: older
            timestring: '%Y.%m.%d'
            unit: days
            unit_count: ${DATA_RETENTION}
            field:
            stats_result:
            epoch:
            exclude: False

# master node configs
elasticsearch:
  enabled: false
  image: "docker.elastic.co/elasticsearch/elasticsearch"
  annotations:
    kubernetes.io/tls-acme: true
    disco: true
  exporter:
    enabled: true
    prometheus: infra-frontend

  roles:
    master: "true"
    ingest: "false"
    data: "false"
  replicas: 3
  esJavaOpts: "-Xmx2G -Xms2G"
  esConfig:
    elasticsearch.yml: |
      indices.query.bool.max_clause_count: 8192
      search.max_buckets: 250000
      xpack.security.enabled: false

    log4j2.properties: |
      logger.rorignore.name = tech.beshu.ror.es.request.queries
      logger.rorignore.level = error
      logger.rorignore.additivity = false

  volumeClaimTemplate:
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 1G
        
  extraInitContainers: |
    - name: configure-ownership
      securityContext:
        runAsUser: 0
        privileged: true
      image: "{{ .Values.image }}:{{ .Values.imageTag }}"
      imagePullPolicy: "{{ .Values.imagePullPolicy }}"
      command: ["sh", "-c", "mkdir -p /usr/share/elasticsearch/data && chown -R 1000:1000 /usr/share/elasticsearch/data"]
      volumeMounts:
        - name: "{{ template "elasticsearch.uname" . }}"
          mountPath: /usr/share/elasticsearch/data

elasticsearch-data:
  enabled: false
  nodeGroup: data
  masterService: elastiflow-master
  roles:
    master: "false"
    ingest: "true"
    data: "true"
  replicas: 2
  esJavaOpts: "-Xmx10G -Xms10G"
  resources:
    requests:
      cpu: "4000m"
      memory: "16Gi"
    limits:
      cpu: "6000m"
      memory: "20Gi"
  esConfig:
    elasticsearch.yml: |
      indices.query.bool.max_clause_count: 8192
      search.max_buckets: 250000
      xpack.security.enabled: false

    log4j2.properties: |
      logger.rorignore.name = tech.beshu.ror.es.request.queries
      logger.rorignore.level = error
      logger.rorignore.additivity = false

  volumeClaimTemplate:
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 500G

  extraInitContainers: |
    - name: configure-ownership
      securityContext:
        runAsUser: 0
        privileged: true
      image: "{{ .Values.image }}:{{ .Values.imageTag }}"
      imagePullPolicy: "{{ .Values.imagePullPolicy }}"
      command: ["sh", "-c", "mkdir -p /usr/share/elasticsearch/data && chown -R 1000:1000 /usr/share/elasticsearch/data"]
      volumeMounts:
        - name: "{{ template "elasticsearch.uname" . }}"
          mountPath: /usr/share/elasticsearch/data
  exporter:
    enabled: true
    prometheus: infra-frontend

logstash:
  enabled: false
  image: "elastiflow-logstash"
  imageTag: "DEFINED-IN-PIPELINE"
  logstashJavaOpts: "-Xmx8G -Xms8G"

  persistence:
    enabled: false

  resources:
    requests:
      cpu: 4000m
      memory: 8G
    limits:
      cpu: 4000m
      memory: 10G

  elastic:
    shards: 3
    replicas: 1
  extraEnvs:
    - name: ELASTIFLOW_DNS_HIT_CACHE_TTL
      value: "604800"
    - name: ELASTIFLOW_RESOLVE_IP2HOST
      value: "exporters"
  jdbc:
    enabled: false
    configMap: "jdbc-filter"
    configFile: "20_filter_95_enrich_ips.conf"
    schedule: "30 1 * * *"
  service:
    # type: NodePort
    # externalTrafficPolicy: Local
    clusterIP: None
    ports:
      - name: beat-tcp
        port: 5044
        protocol: TCP
        targetPort: 5044
      # - name: netflow-udp
      #   port: 2055
      #   targetPort: 2055
      #   nodePort: 30055
      #   protocol: UDP
      # - name: sflow-udp
      #   port: 6343
      #   targetPort: 6343
      #   nodePort: 30343
      #   protocol: UDP
      # - name: ipfix-udp
      #   port: 4739
      #   targetPort: 4739
      #   nodePort: 30739
      #   protocol: UDP
      # - name: ipfix-tcp
      #   port: 4739
      #   targetPort: 4739
      #   nodePort: 30740
      #   protocol: TCP
      # - name: http
      #   port: 9600
      #   targetPort: 9600
      #   protocol: TCP
  alerts:
    enabled: false
    prometheus: infra-collector

  readinessProbe:
    # use beats port instead of http API to better check elastiflow specific status
    httpGet: null
    tcpSocket:
      port: 5044
    initialDelaySeconds: 180
    periodSeconds: 30
    failureThreshold: 20
    successThreshold: 1

  livenessProbe:
    httpGet: null
    tcpSocket:
      port: 5044
    initialDelaySeconds: 600
    periodSeconds: 20
    failureThreshold: 5
    successThreshold: 1

logstashExporter:
  name: elastiflow
  enabled: false
  image: 
    repo: bonniernews/logstash_exporter
    tag: v0.1.2
  ls:
    uri: http://elastiflow-logstash:9600
  listen_port: 9198
  targets: infra-collector

filebeat:
  enabled: false
  image: "elastiflow-filebeat"
  imageTag: "20210709141141"
  externalIPs:

  service:
    type: NodePort
    externalTrafficPolicy: Local
    ports:
      - name: netflow-udp
        port: 2055
        targetPort: 2055
        protocol: UDP
  resources:
    requests:
      cpu: "2000m"
      memory: "6G"
    limits:
      cpu: "4000m"
      memory: "16G"

kibana:
  enabled: false
  image: "docker.elastic.co/kibana/kibana"
  elasticsearchHosts: "http://elastiflow-master:9200"
  # resources:
  #   requests:
  #     memory: 4Gi
  #   limits:
  #     memory: 5Gi

# Deploy Prometheus alerts.
alerts:
  enabled: false
  # Name of the Prometheus to which the alerts should be assigned to.
  prometheus: infra-frontend

queryExporter:
  name: elastiflow
  enabled: false
  image:
    repo: braedon/prometheus-es-exporter
    tag: 0.9.0
  es:
    uri: "http://elasticsearch-master:9200"
  listen_port: 9206
  targets: infra-frontend
  config:
    exporter.cfg: |
      [DEFAULT]
      QueryIntervalSecs = 15
      QueryTimeoutSecs = 10
      QueryIndices = _all
      QueryOnError = drop
      QueryOnMissing = drop

      [query_all]
      QueryJson = {"size": 0, "query": {"match_all": {}}}
