groups:
- name: kubernikus-kluster.alerts
  rules:
  - alert: KubernikusKlusterStuck
    expr: kubernikus_kluster_status_phase{phase!="Running"} == 1
    for: 1h
    labels:
      tier: kks
      service: ground
      support_group: containers
      severity: info
      meta: "{{ $labels.kluster_id }} stuck in {{ $labels.phase }}"
    annotations:
      description: Kluster {{ $labels.kluster_id }} is stuck in {{ $labels.phase }} for 1h
      summary: Kluster stuck in phase {{ $labels.phase }}

  - alert: KubernikusKlusterStuckCreating
    expr: kubernikus_kluster_status_phase{phase="Creating"} == 1
    for: 1h
    labels:
      tier: kks
      service: ground
      support_group: containers
      severity: warning
      meta: "{{ $labels.kluster_id }} stuck in {{ $labels.phase }}"
    annotations:
      description: Kluster {{ $labels.kluster_id }} is stuck in {{ $labels.phase }} for 1h
      summary: Kluster stuck in phase {{ $labels.phase }}

  - alert: KubernikusKlusterCreationBroken
    expr: count(max(sum_over_time(kubernikus_kluster_status_phase{phase="Creating"}[4h])) by (kluster_id,region) > 10) by (region) > 0
    for: 1h
    labels:
      tier: kks
      service: ground
      support_group: containers
      severity: info
      meta: "Investigate kubernikus control-plane {{ $labels.cluster }}"
      playbook: docs/support/playbook/kubernikus/kubernikus_kluster_creation_broken
    annotations:
      description: Klusters can't be created within 10min. There is a problem with the kubernikus control-plane {{ $labels.cluster }}. Most likely because volumes don't attach in time. Login to {{ $labels.cluster }} and investiage why pods in the `kubernikus` namespace can't start.
      summary: Klusters can't be created within 10min. There is a problem with the kubernikus control-plane {{ $labels.cluster }}.

  - alert: KubernikusKlusterUnavailable
    expr: min(probe_success{kubernetes_namespace="kubernikus"} != 1) by (kubernetes_name) and min(label_replace(kubernikus_kluster_status_phase{phase="Running"}== 1, "kubernetes_name", "$1", "kluster_id", "(.*)")) by (kubernetes_name)
    for: 20m
    labels:
      tier: kks
      service: kubernikus
      support_group: containers
      severity: warning
      meta: "{{ $labels.kubernetes_name }} is unavailable"
    annotations:
      description: "{{ $labels.kubernetes_name }} is unavailable since 10m"
      summary: "{{ $labels.kubernetes_name }} is unavailable"

  - alert: KubernikusKlusterLowOnObjectStoreQuota
    expr: label_replace(max(kubernikus_kluster_info{phase="Running", backup="swift"}) by (kluster_name, creator, project_id), "shortname", "$1", "kluster_name", "(.*)-[a-f0-9]{32}") * on(project_id) group_left(project, domain) (min(limes_free_swift_quota_gauge) by (project_id, project, domain) < 90 * 1024 * 1024)
    for: 10m
    labels:
      tier: kks
      service: kubernikus
      support_group: containers
      severity: warning
      meta: "Kluster {{ $labels.kluster_name }} is low on object-store quota"
      playbook: docs/support/playbook/kubernikus/low_object_store_quota
      kluster_name: "{{ $labels.kluster_name }}"
    annotations:
      summary: "Kluster {{ $labels.kluster_name }} is low on object-store quota"
      description: "Kluster {{ $labels.kluster_name }} in `{{ $labels.domain }}/{{ $labels.project }}` has *{{ humanize1024 $value }}B* of object-store quota left. If the quota is exhausted the etcd backup of the cluster will fail. This can also cause an outage. The project admin needs to raise quota or free up space. <https://dashboard.{{ $externalLabels.region }}.cloud.sap/_/{{ $labels.project_id }}/masterdata-cockpit/project|Masterdata>, cluster creator:  <https://people.wdf.sap.corp/profiles/{{$labels.creator}}|{{$labels.creator}}>"
      mail_subject: "ACTION REQUIRED: Object store capacity low in project {{ $labels.domain }}/{{ $labels.project }} in {{ $externalLabels.region }}" 
      mail_body: |
        Dear Converged Cloud customer,
        the project {{ $labels.domain }}/{{ $labels.project }} has {{ humanize1024 $value }}B of object-store capacity left.

        https://dashboard.{{ $externalLabels.region }}.cloud.sap/_/{{ $labels.project_id }}/object-storage/

        The kubernetes cluster {{ $labels.shortname }} relies on the object store to continously backup its etcd data.
        If the quota is exhausted the backup will fail and recovering the cluster from data loss or corruption will be impossible.
        For the correct functionining of the provisioned cluster please make sure you always have >200MB of object-store capacity available.

        Kind regards,
        Converged Cloud support team

  - alert: KubernikusSeedReconciliationFailed
    expr: increase(kubernikus_seed_reconciliation_failures_total[35m])>6
    for: 10m
    labels:
      tier: kks
      service: ground
      support_group: containers
      severity: warning
      meta: "Seed reconciliation is failing for kluster {{ $labels.kluster_name }}"
    annotations:
      description: "Seed reconciliation is failing for kluster {{ $labels.kluster_name }}. Check Kubernikus operator logs in k-controlplane kluster for details, eg. `kubectl --context={{ $labels.cluster }} -n kubernikus-system logs -l type=operator | grep {{ $labels.kluster_name }}`"
      summary: Seed reconciliation failed
