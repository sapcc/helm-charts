owner-info:
  support-group: containers
  helm-chart-url: https://github.com/sapcc/helm-charts/tree/master/system/cc-ceph

rook-ceph:
  monitoring:
    enabled: true
  crds:
    enabled: false

ceph:
  registry:
  image: "ceph/ceph"
  version:

cephConfig:
  global:
    bdev_flock_retry: "20"
    bluefs_buffered_io: "false"
    mon_allow_pool_delete: "false"
    mon_allow_pool_size_one: "false"
    osd_pool_default_size: "3"
    mon_data_avail_warn: "10"
    osd_op_queue: "wpq"
    osd_pool_default_pg_autoscale_mode: "off"
  mon:
    ms_client_mode: "secure"
    ms_cluster_mode: "secure"
    ms_service_mode: "secure"
    ms_mon_cluster_mode: "secure"
    ms_mon_client_mode: "secure"
    ms_mon_service_mode: "secure"

clusterName: ceph-cluster

mon:
  count: 5
  nodeRole: ceph-mon

resources:
  osd:
    limits:
      memory: 8192Mi #no limits for osds leads to osd taking all available memory
    requests:
      memory: 8192Mi

osd:
  nodeRole: ceph-osd

prepareosd:
  nodeRole: ceph-osd

pool:
  mgr:
    replicated: 3
  # Sets "ceph osd pool set nodelete" to all CephBolckPools. Valid values: "0" | "1"
  # Set to "1" to to prevent unintended pool changes in production
  nodelete: "1"
  # Sets "ceph osd pool set nodelete" to all CephBolckPools. Valid values: "0" | "1"
  # Set to "1" to to prevent unintended pool changes in production
  nosizechange: "1"

mgr:
  nodeRole: ceph-mon

storage:
  useAllNodes: false
  useAllDevices: false
  deviceFilter: "^sd[a-q]+$"
  config:
    osdsPerDevice: "1"
  #  metadataDevice: "nvme0n1"
  #  databaseSizeMB: "120000"
    encryptedDevice: "true"
  nodes: null
  # - name: a
  #   config:
  #     metadataDevice: "nvme0n1"
  # - name: b
  #   config:
  #     metadataDevice: "nvme1n1"
  # - name: c
  #   config:
  #     metadataDevice: "nvme0n1"

dashboard:
  ingress:
    host: "mydash.local.domain"

objectstore:
  enabled: true
  name: objectstore
  enabledAPIs: [] # empty - all enabled. See: https://docs.ceph.com/en/reef/radosgw/config-ref/#confval-rgw_enable_apis
  gateway:
    instances: 6
    port: 80
    securePort: 443
    sslCertificateRef: ""
    dnsNames: []
    # - dns1
    # - dns2
    resources:
      requests:
        cpu: 10
        memory: 16Gi
  metadataPool:
    # replicated:
    #   size: 4
    #   replicasPerFailureDomain: 2
    #   subFailureDomain: "host"
    # failureDomain: "zone"
    # deviceClass: "nvme"
  dataPool:
    # choose between replicated and erasureCoded
    # erasureCoded:
    #   codingChunks: 1
    #   dataChunks: 2
    # failureDomain: host
    # replicated:
    #   size: 3
  service:
    name: ceph-objectstore-external
    port: 443
    externalIPs:
      - 10.0.0.1
  keystone:
    enabled: true
    global_config: true
    auth_order:
      - local
      - external
    url: http://keystone.local:8083
    verify_ssl: true
    implicit_tenants: true
    swift_account_in_url: true
    swift_versioning_enabled: true
    accepted_roles:
      - objectstore_admin
    accepted_admin_roles:
      - cloud_objectstore_admin
    token_cache_size: 1000
    admin_user: XXX
    admin_password: XXX
    admin_domain: XXX
    admin_project: XXX
    barbican:
      enabled: true
      url: http://barbican.local:9311
      user: XXX
      password: XXX
      domain: XXX
      project: XXX
  multiInstance:
    enabled: false
    extraInstances:
      # can inherit/override all config options from objectstore:
    # - name: objectstore-admin
    #   gateway:
    #     instances: 2
    #     sslCertificateRef: ""
    #     dnsNames:
    #     - dns1-adm
    #     - dns2-adm
    #     resources:
    #       requests:
    #         cpu: 1
    #         memory: 2Gi
    #   service:
    #     name: ceph-objectstore-admin-external
    #     port: 443
    #     externalIP: "10.0.0.1"
    #     selector:
    #       app: rgw
    # define other RGW instances here:
    # - name: other-instance-name

  prysm:
    initContainer:
      repository:
        image: "k8s.gcr.io/kubectl"
        tag: "latest"
        pullPolicy: "Always"
    repository:
      image: "ghcr.io/cobaltcore-dev/prysm"
      tag: "latest"
      pullPolicy: "Always"
    rgwMetrics:
      enabled: true
      interval: "10"
      user:
        name: prysm-user
        # name of rgw instance with admin-ops api enabled
        # default is objectstore.name
        store:
    diskHealth:
      enabled: true
      interval: "60"
      disks: "*"
      nodeSelector:
        kubernetes.metal.cloud.sap/role: ceph-osd

  prysmDiskHealth:
    repository:
      image: "ghcr.io/cobaltcore-dev/prysm"
      tag: "latest"
      pullPolicy: "Always"

rgwTargetPlacements:
  #  enabled: false deprecate rgwTargetPlacements.enabled because it is true on all envs
  useRookCRD: false # !!!WARNING set 'true' only for new clusters. Upgrade will not work now.

blockrbdpool:
  enabled: false
  - name: rbd-region-premium
    size: 3
    failureDomain: zone
    crushRoot: default

# a list of dnsNames to skip DNS record creation for RGW instances
dnsNamesSkipRecord: []
# a list of dnsNames to skip DNS record creation for RGW instances with wildcard
dnsNamesSkipRecordWildcard: []
# a list of dnsNames to skip certificate creation for RGW instances
dnsNamesSkipCertificate: []
# a list of dnsNames to skip certificate creation for RGW instances with wildcard
dnsNamesSkipCertificateWildcard: []
