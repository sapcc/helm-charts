apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: {{ .Values.objectstore.name }}
  namespace: {{ .Release.Namespace }}
spec:
  metadataPool: {{ toYaml .Values.objectstore.metadataPool | nindent 4 }}
  dataPool: {{ toYaml .Values.objectstore.dataPool | nindent 4 }}
  hosting:
    dnsNames: {{ toYaml .Values.objectstore.gateway.dnsNames | nindent 8 }}
  gateway:
    annotations:
      checksum/config: {{ include (print $.Template.BasePath "/ceph-config-override.yaml") . | sha256sum }}
    instances: {{ .Values.objectstore.gateway.instances }}
    {{- if .Values.objectstore.gateway.port }}
    port: {{ .Values.objectstore.gateway.port }}
    {{- end }}
    {{- if .Values.objectstore.gateway.securePort }}
    securePort: {{ .Values.objectstore.gateway.securePort }}
    {{- end }}
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.metal.cloud.sap/role
                  operator: In
                  values:
                    - {{ .Values.osd.nodeRole }}
      # since the CephCluster's network provider is "host", we need to isolate 80/443 port listeners from each other
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - rook-ceph-rgw
          topologyKey: kubernetes.io/hostname
    priorityClassName: system-cluster-critical
    sslCertificateRef: {{ .Values.objectstore.gateway.sslCertificateRef }}
    resources: {{ toYaml .Values.objectstore.gateway.resources | nindent 6 }}
  preservePoolsOnDelete: true
