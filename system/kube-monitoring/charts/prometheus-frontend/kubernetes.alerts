groups:
- name: kubernetes.alerts
  rules:
  - alert: KubernetesNodeManyNotReady
    expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0) > 2
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
      dashboard: kubernetes-health
    annotations:
      description: Many Nodes are NotReady
      summary: Many ({{$value}}) nodes are NotReady for more than an hour

  - alert: KubernetesNodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
      dashboard: kubernetes-node?var-server={{$labels.instance}}
      playbook: docs/support/playbook/k8s_node_not_ready.html
    annotations:
      description: Node status is NotReady
      summary: Node {{$labels.node}} is NotReady for more than an hour

  - alert: KubernetesNodeNotReadyFlapping
    expr: changes(kube_node_status_condition{condition="Ready",status="true"}[15m]) > 2
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: node
      meta: "{{ $labels.instance }}"
      dashboard: "kubernetes-node?var-server={{$labels.instance}}"
    annotations:
      description: Node readiness is flapping
      summary: Node {{$labels.node}} is flapping between Ready and NotReady

  - alert: KubernetesNodeScrapeMissing
    expr: absent(up{job="endpoints",kubernetes_name="kube-state-metrics"})
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: node
      dashboard: kubernetes-health
    annotations:
      description: Node status cannot be scraped
      summary: Node status failed to be scraped

  - alert: KubernetesPodRestartingTooMuch
    expr: rate(kube_pod_container_status_restarts[15m]) > 0
    for: 1h
    labels:
      tier: kubernetes
      service: resources
      severity: info
      context: pod
      meta: "{{$labels.namespace}}/{{$labels.pod}}"
    annotations:
      description: Pod is in a restart loop
      summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting constantly

  - alert: KubernetesPodRestartingTooMuch
    expr: rate(kube_pod_container_status_restarts[15m]) > 0
    for: 1h
    labels:
      tier: kubernetes
      service: resources
      severity: info
      context: pod
      meta: "{{$labels.namespace}}/{{$labels.pod}}"
    annotations:
      description: Pod is in a restart loop
      summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting constantly

  - alert: KubernetesSchedulerDown
    expr: count(up{job="kube-system/scheduler"} == 1) == 0
    for: 5m
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: scheduler
      dashboard: kubernetes-health
    annotations:
      description: Scheduler is down
      summary: No scheduler is running. New pods are not being assigned to nodes!

  - alert: KubernetesSchedulerScrapeMissing
    expr: absent(up{job="kube-system/scheduler"})
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: scheduler
      dashboard: kubernetes-health
    annotations:
      description: Scheduler in failed to be scraped
      summary: Scheduler cannot be scraped

  - alert: KubernetesControllerManagerDown
    expr: count(up{job="kube-system/controller-manager"} == 1) == 0
    for: 5m
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: controller-manager
      dashboard: kubernetes-health
    annotations:
      description: No controller-manager is running. Deployments and replication controllers are not making progress
      summary: Controller manager is down

  - alert: KubernetesControllerManagerDown
    expr: count(up{job="kube-system/controller-manager"} == 1) == 0
    for: 5m
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: controller-manager
      dashboard: kubernetes-health
    annotations:
      description: No controller-manager is running. Deployments and replication controllers are not making progress
      summary: Controller manager is down

  - alert: KubernetesControllerManagerScrapeMissing
    expr: absent(up{job="kube-system/controller-manager"})
    for: 1h
    labels:
      tier: kubernetes
      service: k8s
      severity: critical
      context: controller-manager
      dashboard: kubernetes-health
    annotations:
      description: ControllerManager failed to be scraped
      summary: ControllerManager cannot be scraped

  - alert: KubernetesTooManyOpenFiles
    expr: 100*process_open_fds{job=~"kube-system/kubelet|kube-system/apiserver"} / process_max_fds > 50
    for: 10m
    labels:
      tier: kubernetes
      service: k8s
      severity: warning
      context: system
      meta: "{{ $labels.instance }}"
      dashboard: kubernetes-node?var-server={{$labels.instance}}
    annotations:
      description: "{{ $labels.job }} on {{ $labels.instance }} is using {{ $value }}% of the available file/socket descriptors"
      summary: Too many open file descriptors
