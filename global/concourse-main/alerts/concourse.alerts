# vim: set ft=yaml:

groups:
- name: concourse.alerts
  rules:
  - alert: ConcoursePostgresDatabaseSize
    expr: pg_database_size_bytes{datname="concourse"} >= 40 * 1024^3
    for: 5m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Database {{ $labels.datname }} to large"
    annotations:
      description: "The concourse database size is greater than 40GB and will exceed the PV limits eventually"
      summary: Concourse database is too large

  - alert: ConcoursePostgresSharedMemoryError
    expr: increase(elasticsearch_database_memory_allocation_failed_doc_count{failed="concourse-postgresql-0"}[6h]) > 0
    for: 5m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Database could not resize shared memory segment error occured"
    annotations:
      description: "Memory pressure on node led to postgres database could not resize shared memory segment error in cluster {{ $labels.cluster }} in pod concourse/{{ $labels.failed }}. This could trigger random pipeline execution. Make sure database is running on a node with enough free memory or scale down concourse web component."
      summary: Concourse database shared memory error

  - alert: ConcourseWorkerStalled
    expr: changes(concourse_workers_containers{platform="linux", worker=~"mo-.*"}[5m])==0
    for: 30m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Worker {{ $labels.name }} has a stale container metric."
    annotations:
      description: "The workers container count seems to be stuck indicating that the worker might by stalled."
      summary: Worker has stale container count

  - alert: ConcourseGitProxyWorkersFailing
    expr: sum(rate(request_duration_seconds_count{code="500", app="git-resource-proxy"}[5m])) by (pod) > 0
    for: 30m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Git-proxy resource check request error rate to high"
    annotations:
      description: "The check requests from the git-proxy resource are failing. This might be because github.wdf.sap.corp is down. Otherwise inspect the logs of git-proxy replicas in the concourse installation. Restarting the pods might help."
      summary: Git-proxy resource checks are failing

  - alert: ConcourseGitProxyWorkerStalled
    # NOTE: ci3 is getting so little traffic that this alert misfires all the time.
    expr: sum(rate(request_duration_seconds_count{cluster!~"ci3-.*"}[5m])) by (pod) == 0
    for: 30m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Git-proxy worker {{ $labels.pod }} is stalled"
    annotations:
      description: "The git-proxy worker {{ $labels.pod }} seems stalled. Check its logs in the concourse namespace."
      summary: Git-proxy worker stalled

  - alert: ConcourseWorkerNodeFullWorkdir
    expr: min by (node) (predict_linear(node_filesystem_free_bytes{mountpoint="/",node=~"kks-ci-.*-ci.*"}[1d], 2 * 24 * 3600) < 0) * on (node) kube_node_info
    for: 30m
    labels:
      tier: ci
      service: concourse
      severity: warning
      context: concourse
      support_group: containers
      meta: "Worker node {{ $labels.node }} workdir is most likely out of disk space in 2 days"
    annotations:
      description: "Worker node `{{ $labels.node }}` workdir is most likely out of disk space in 2 days. Drain and terminate node to create a worker with empty workdir. Node creation and volume provisioning will happen automatically."
      summary: "Worker node workdir out of disk space in the near future"
