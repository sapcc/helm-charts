{{ if .Values.gardenlet.enabled -}}
{{- range $index, $seedRegion := .Values.gardenlet.seedRegions }}
{{- $regionName := default $seedRegion.name $seedRegion.altRegion }}
{{- $clusterName := printf "%s-%s" (required "Values.gardenlet.namePrefix is required" $.Values.gardenlet.namePrefix) $seedRegion.name }}
---
apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: Gardenlet
metadata:
  name: {{ $clusterName }}
  namespace: garden
spec:
{{- if $seedRegion.bootstrap}}
  kubeconfigSecretRef:
    name: secret-to-be-patched-manually
{{- end }}
  deployment:
    image:
      tag: {{ $.Chart.AppVersion }}
    imageVectorOverwrite: |
{{ $.Values.operator.imageVectorOverwrite | indent 6 }}
    podLabels:
      networking.resources.gardener.cloud/to-virtual-garden-kube-apiserver-tcp-443: allowed
    helm:
      ociRepository:
        ref: keppel.global.cloud.sap/ccloud-europe-docker-pkg-dev-mirror/gardener-project/releases/charts/gardener/gardenlet:{{ $.Chart.AppVersion }}
  config:
    gardenClientConnection:
      kubeconfigSecret:
        name: gardenlet-kubeconfig
        namespace: garden
      kubeconfigValidity:
        validity: 72h
    logging:
      enabled: true
    seedConfig:
      metadata:
        labels:
          environment: production
{{- if $.Values.extensions.metal.enabled }}
        annotations:
          metal.ironcore.dev/local-metal-api: "true"
{{- end }}
      spec:
        backup:
          provider: {{ default "aws" $seedRegion.backup.provider | quote }}
          region: {{ default $regionName $seedRegion.backup.region | quote }}
          credentialsRef:
            apiVersion: v1
            kind: Secret
            {{- if eq "aws" $seedRegion.backup.provider }}
            name: {{ (printf "aws-backup-%s" $seedRegion.name) }}
            {{- else }}
            name: {{ (printf "openstack-%s" $regionName) }}
            {{- end }}
            namespace: garden
        dns:
          defaults:
          - credentialsRef:
              apiVersion: v1
              kind: Secret
              name: openstack-{{ $regionName }}
              namespace: garden
            domain: {{ default (printf "external.%s.%s.%s.cloud.sap" $clusterName (ternary "soil-customer" "soil-garden" (hasPrefix "rtc-" $clusterName)) $seedRegion.name) $seedRegion.externalDomain }}
            type: openstack-designate
          internal:
            credentialsRef:
              apiVersion: v1
              kind: Secret
              name: openstack-{{ $regionName }}
              namespace: garden
            domain: {{ default (printf "internal.%s.%s.%s.cloud.sap" $clusterName (ternary "soil-customer" "soil-garden" (hasPrefix "rtc-" $clusterName)) $seedRegion.name) $seedRegion.internalDomain }}
            type: openstack-designate
          provider:
            type: openstack-designate
            secretRef:
              name: openstack-{{ $regionName }}
              namespace: garden
        ingress: # see prerequisites
          domain: {{ default (printf "ingress.%s.%s.%s.cloud.sap" $clusterName (ternary "soil-customer" "soil-garden" (hasPrefix "rtc-" $clusterName)) $seedRegion.name) $seedRegion.ingressDomain }}
          controller:
            kind: nginx
        networks:
          nodes: {{ $seedRegion.networks.nodes | quote }}
          pods: {{ $seedRegion.networks.pods | quote }}
          services: {{ $seedRegion.networks.services | quote }}
        provider:
          region: {{ default $seedRegion.name $seedRegion.providerRegion }}
          type: openstack
          zones: {{ toYaml $seedRegion.zones | nindent 12 }}
        settings:
          verticalPodAutoscaler:
            enabled: false
          excessCapacityReservation:
            enabled: false
          dependencyWatchdog:
            # The prober can start a vicious downscaling cycle from which a shoot cannot recover itself.
            # Let's assume there is healthy shoot and a machine is added.
            # This machine now fails to join the cluster temporarly for whatever reason.
            # This causes the prober to kick in and it will scale down the controller-manager as well as the MCM.
            # The node now gains network and it's kubelet tries the TLS bootstap.
            # The bootstrap does not go through, because the kubelets CertificateSigningRequest is never approved, because the controller-manager is scaled down.
            # Joining the node successfully is unfortunately required to scale up the controller-manager, which is a loop that cannot self-heal.
            prober:
              enabled: false
{{- if $.Values.gardenlet.loadBalancerServices }}
          loadBalancerServices: {{ toYaml $.Values.gardenlet.loadBalancerServices | nindent 12 }}
{{- end }}
{{- end }}
{{ end -}}
