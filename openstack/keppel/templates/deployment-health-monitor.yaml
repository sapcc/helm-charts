kind: Deployment
apiVersion: apps/v1

metadata:
  name: keppel-health-monitor

spec:
  revisionHistoryLimit: 5
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 2
  selector:
    matchLabels:
      name: keppel-health-monitor
  template:
    metadata:
      labels:
        name: keppel-health-monitor
      annotations:
        checksum/secret: {{ include "keppel/templates/secret.yaml" . | sha256sum }}
        kubectl.kubernetes.io/default-container: monitor
    spec:
      containers:
        - name: monitor
          image: {{include "keppel_image" .}}
          imagePullPolicy: IfNotPresent
          args: [ server, healthmonitor, {{quote .Values.keppel.healthcheck_account}} ]
          env:
            - name:  KEPPEL_DEBUG
              value: 'false'
            - name: OS_AUTH_URL
              value: "http://keystone.{{ .Values.global.keystoneNamespace }}.svc.kubernetes.{{ .Values.global.region }}.{{ .Values.global.tld }}:5000/v3"
            - name:  OS_AUTH_VERSION
              value: '3'
            - name:  OS_IDENTITY_API_VERSION
              value: '3'
            - name:  OS_INTERFACE
              value: internal
            - name:  OS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: keppel-secret
                  key: service_user_password
            - name:  OS_PROJECT_DOMAIN_NAME
              value: 'Default'
            - name:  OS_PROJECT_NAME
              value: 'service'
            - name:  OS_REGION_NAME
              value: {{ quote $.Values.global.region }}
            - name:  OS_USER_DOMAIN_NAME
              value: 'Default'
            - name:  OS_USERNAME
              value: 'keppel'
          securityContext:
            runAsNonRoot: true
          ports:
            - name: http
              containerPort: 8080
          # We don't want the livenessProbe to kill us excessively often. The
          # healthmonitor can well operate in a non-Ready state if Keppel is
          # down. The only reason to actively restart it is to try and
          # re-execute the startup phase where the test image gets uploaded.
          # Otherwise, we may get stuck in a loop of trying to pull a test
          # image that is known broken and will stay this way.
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            initialDelaySeconds: 300 # do not restart uselessly often
            timeoutSeconds: 5
            periodSeconds: 5
            failureThreshold: 3 # but restart quickly once the 300 seconds are up
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8080
            timeoutSeconds: 5
            periodSeconds: 5
          # NOTE: observed resource usage in eu-de-1 (as of 2020-02-26)
          # - CPU: spiky, but always below 1m
          # - memory: usually level around 17 Mi
          resources:
            requests:
              cpu: "10m"
              memory: "64Mi"
            limits:
              cpu: "10m"
              memory: "64Mi"
