# vim: set ft=yaml:

groups:
- name: openstack-keppel-api.alerts
  rules:

  - alert: OpenstackKeppelDown
    expr: max(keppel_healthmonitor_result) != 1 or absent(keppel_healthmonitor_result)
    for: 10m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: critical
      support_group: containers
      tier: os
      meta: 'Keppel health check is not reporting success'
      playbook: docs/support/playbook/keppel/down
    annotations:
      summary: Keppel health check is not reporting success.
      description: |
        Keppel health check is not reporting success. Check that the
        "keppel-health-monitor" pod in the Kubernetes namespace "keppel" is
        running, and if so, check its log for error messages.

  # This alert is disabled for QA because the lab regions are literal testbeds
  # and therefore have connectivity issues all the time.
  - alert: OpenstackKeppelAnycastDown
    expr: max by (account) (keppel_anycastmonitor_result{region!~"qa-.*"}) != 1
    for: 30m # triggers rather slowly because it does not make sense to ping the Keppel team for short-term network issues
    labels:
      context: anycast-api
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      # meta is part of an alertmanager inhibition rule
      meta: 'Keppel anycast health check failing for {{ $labels.account }}'
    annotations:
      summary: 'Keppel anycast health check failing for {{ $labels.account }}'
      description: |
        The Keppel anycast health check is failing for some peers.
        If there are other alerts for {{ $labels.account }} itself being down, this is probably an aftereffect; please investigate the broken region first.
        If only anycast is broken, check the logs of the "keppel-anycast-monitor" pod for details.

  # This alert is disabled for QA because the QA anycast looks kind of broken.
  - alert: OpenstackKeppelAnycastMembership
    expr: max(keppel_anycastmonitor_membership{region!~"qa-.*"}) != 1
    for: 10m
    labels:
      context: anycast-membership
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      meta: 'Keppel anycast membership check failing'
    annotations:
      summary: 'Keppel anycast membership check failing'
      description: |
        When talking to the Keppel anycast endpoint from within the Keppel
        deployment, we don't reach our own API. If this alert occurs by
        itself, without any other alerts accompanying it, this Keppel has most
        likely not been added to the anycast configuration yet. Check the logs
        of the "keppel-anycast-monitor" pod for details.

  - alert: OpenstackKeppelSlowPeering
    expr: time() - max by (hostname) (keppel_peers_last_peered_at) > 1800
    for: 5m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      # meta is part of an alertmanager inhibition rule
      meta: 'Keppel cannot peer with {{ $labels.hostname }}'
    annotations:
      summary: 'Keppel cannot peer with {{ $labels.hostname }}'
      description: |
        The Keppel instance in this region should check in with its peer
        {{ $labels.hostname }} every 10 minutes, but it has not done so in
        the last 30 minutes. The logs of the keppel-api pods should contain
        additional information or error messages.

  - alert: OpenstackKeppelSlowBlobReplication
    expr: max(keppel_blob_replication_min_started_at) > 0 and time() - max(keppel_blob_replication_min_started_at) > 900
    for: 5m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      meta: 'Blob replication is taking a long time'
    annotations:
      summary: 'Blob replication is taking a long time'
      description: |
        A blob replication has been running in this Keppel instance for more
        than 15 minutes. This could indicate that the region interconnect is slow.
        Check the "pending_blobs" DB table to see which blobs are affected.
        Entries older than a few hours can usually be deleted safely in order
        to allow another replication to take place.

  - alert: OpenstackKeppelDBConnectionPoolNearlyFull
    expr: max by (pod) (avg_over_time(go_sql_stats_connections_in_use{db_name="keppel"}[1h]) > 12)
    for: 5m
    labels:
      context: dbconnpool
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      meta: 'DB connection pool nearly full on {{ $labels.pod }}'
    annotations:
      summary: 'DB connection pool nearly full on {{ $labels.pod }}'
      description: |
        The DB connection pool on pod {{ $labels.pod }} is filling up. It can
        go up to 16 connections, but during regular operations we should not go
        over 3-5 connections to retain some budget for request spikes. Going
        high on connections for a long time indicates that the pod might be
        starved for CPU time, so try checking the CPU throttling metrics.

  - alert: OpenstackKeppelAuditEventPublishFailing
    # Usually, you would check increase() here, but audit events *can* be quite
    # rare, so we alert if there are any failed audit events at all. To clear this alert,
    # delete the respective
    expr: max by (pod) (keppel_failed_auditevent_publish > 0)
    for: 1h
    labels:
      context: auditeventpublish
      dashboard: keppel-overview
      service: keppel
      severity: info
      support_group: containers
      tier: os
      meta: '{{ $labels.pod }}'
    annotations:
      summary: "{{ $labels.pod }} cannot publish audit events"
      description: "Audit events from {{ $labels.pod }} could not be published to the RabbitMQ server. Check the pod log for details. Once the underlying issue was addressed, delete the offending pod to clear this alert."
