# vim: set ft=yaml:

groups:
- name: openstack-keppel-api.alerts
  rules:

  - alert: OpenstackKeppelDown
    expr: keppel_healthmonitor_result != 1 or absent(keppel_healthmonitor_result)
    for: 3m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: warning # TODO raise to critical at some point
      tier: os
      meta: 'Keppel health check is not reporting success'
      playbook: docs/support/playbook/keppel/down.html
    annotations:
      summary: Keppel health check is not reporting success.
      description: |
        Keppel health check is not reporting success. Check that the
        "keppel-health-monitor" pod in the Kubernetes namespace "keppel" is
        running, and if so, check its log for error messages.

  # This alert is disabled for QA because the lab regions are literal testbeds
  # and therefore have connectivity issues all the time.
  - alert: OpenstackKeppelAnycastDown
    expr: keppel_anycastmonitor_result{region!~"qa-.*"} != 1
    for: 30m # triggers rather slowly because it does not make sense to ping the Keppel team for short-term network issues
    labels:
      context: anycast-api
      dashboard: keppel-overview
      service: keppel
      severity: info
      tier: os
      meta: 'Keppel anycast health check failing for {{ $labels.account }}'
    annotations:
      summary: 'Keppel anycast health check failing for {{ $labels.account }}'
      description: |
        The Keppel anycast health check is failing for some peers. Check the
        logs of the "keppel-anycast-monitor" pod for details.

  # This alert is disabled for QA because the QA anycast looks kind of broken.
  - alert: OpenstackKeppelAnycastMembership
    expr: keppel_anycastmonitor_membership{region!~"qa-.*"} != 1
    for: 10m
    labels:
      context: anycast-membership
      dashboard: keppel-overview
      service: keppel
      severity: info
      tier: os
      meta: 'Keppel anycast membership check failing'
    annotations:
      summary: 'Keppel anycast membership check failing'
      description: |
        When talking to the Keppel anycast endpoint from within the Keppel
        deployment, we don't reach our own API. If this alert occurs by
        itself, without any other alerts accompanying it, this Keppel has most
        likely not been added to the anycast configuration yet. Check the logs
        of the "keppel-anycast-monitor" pod for details.

  - alert: OpenstackKeppelSlowPeering
    expr: time() - keppel_peers_last_peered_at > 1800
    for: 5m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: info
      tier: os
      meta: 'Keppel cannot peer with {{ $labels.hostname }}'
    annotations:
      summary: 'Keppel cannot peer with {{ $labels.hostname }}'
      description: |
        The Keppel instance in this region should check in with its peer
        {{ $labels.hostname }} every 10 minutes, but it has not done so in
        the last 30 minutes. The logs of the keppel-api pods should contain
        additional information or error messages.

  - alert: OpenstackKeppelSlowBlobReplication
    expr: keppel_blob_replication_min_started_at > 0 and time() - keppel_blob_replication_min_started_at > 900
    for: 5m
    labels:
      context: api
      dashboard: keppel-overview
      service: keppel
      severity: info
      tier: os
      meta: 'Blob replication is taking a long time'
    annotations:
      summary: 'Blob replication is taking a long time'
      description: |
        A blob replication has been running in this Keppel instance for more
        than 15 minutes. This could indicate that the region interconnect is slow.

  - alert: OpenstackKeppelDBConnectionPoolNearlyFull
    expr: avg_over_time(go_sql_stats_connections_in_use{db_name="keppel"}[1h]) > 8
    for: 5m
    labels:
      context: dbconnpool
      dashboard: keppel-overview
      service: keppel
      severity: info
      tier: os
      meta: 'DB connection pool nearly full on {{ $labels.pod }}'
    annotations:
      summary: 'DB connection pool nearly full on {{ $labels.pod }}'
      description: |
        The DB connection pool on pod {{ $labels.pod }} is filling up. It can
        go up to 16 connections, but during regular operations we should not go
        over 3-5 connections to retain some budget for request spikes. Going
        high on connections for a long time indicates that the pod might be
        starved for CPU time, so try checking the CPU throttling metrics.
