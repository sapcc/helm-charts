groups:
- name: openstack-nanny.alerts
  rules:
#   - alert: OpenstackVcenterNannyGhostVolume
#     expr: sum(vcenter_nanny_ghost_volumes) by (kubernetes_name) > 0
#     for: 5m
#     labels:
#       context: nanny
#       service: nanny
#       severity: ignore_info
#       tier: os
#     annotations:
#       description: "The {{ $labels.kubernetes_name }} discovered a ghost volume. a good opportunity to search for the underlying bug now ..."
#       summary: Vcenter nanny detected a ghost volume
#
#   - alert: OpenstackVcenterNannyVolumeAttachmentInconsistency
#     expr: sum(vcenter_nanny_volume_attachment_inconsistencies{region=~"staging|qa-de-1|ap-au-1|ap-ae-1|ap-jp-1|ap-jp-2|eu-ru-1|la-br-1|na-ca-1|na-us-1|na-us-3"}) > 1000
#     for: 60m
#     labels:
#       context: nanny
#       service: nanny
#       severity: ignore_info
#       tier: os
#     annotations:
#       description: "The {{ $labels.kubernetes_name }} discovered a volume with an inconsistent state across nova, cinder and the vcenter - please ignore for now, inconsistencies are now primarily handled by the vcenter-nanny-consistency"
#       summary:  "The {{ $labels.kubernetes_name }} discovered a volume with an inconsistent state across nova, cinder and the vcenter - please ignore for now"

  - alert: OpenstackVcenterNannyConsistencyVolumeAttachingForTooLong
    expr: vcenter_nanny_consistency_cinder_volume_attaching_for_too_long > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'attaching' - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'attaching' - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeDetachingForTooLong
    expr: vcenter_nanny_consistency_cinder_volume_detaching_for_too_long > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'detaching' - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'detaching' - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeCreatingForTooLong
    expr: vcenter_nanny_consistency_cinder_volume_creating_for_too_long > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'creating' - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'creating' - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeDeletingForTooLong
    expr: vcenter_nanny_consistency_cinder_volume_deleting_for_too_long > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'deleting' - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'deleting' - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeIsInStateReserved
    expr: vcenter_nanny_consistency_cinder_volume_is_in_state_reserved > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'reserved' - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'reserved' - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeAvailableWithAttachments
    expr: vcenter_nanny_consistency_cinder_volume_available_with_attachments > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'available' with attachments - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'available' with attachments - should fix itself"

  - alert: OpenstackVcenterNannyConsistencyVolumeInUseWithoutSomeAttachments
    expr: vcenter_nanny_consistency_cinder_volume_in_use_without_some_attachments > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'in-use' with some attachments missing - this should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'in-use' with some attachments missing - should be investigated ..."

  - alert: OpenstackVcenterNannyConsistencyVolumeInUseWithoutAttachments
    expr: vcenter_nanny_consistency_cinder_volume_in_use_without_attachments > 0
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      playbook: 'docs/support/playbook/cinder/error_deleting.html'
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'in-use' with all attachments missing - this problem should be gone automatically in the next nanny run"
      summary: "The {{ $labels.kubernetes_name }} discovered a volume, which is for too long in the state 'in-use' with some attachments missing - should fix itself"

#  - alert: OpenstackVcenterNannyConsistencyTooMuchToFix
#    expr: vcenter_nanny_consistency_cinder_volume_attachment_fix_count > vcenter_nanny_consistency_cinder_volume_attachment_max_fix_count
#    for: 30m
#    labels:
#      context: nanny
#      service: nanny
#      severity: info
#      tier: os
#    annotations:
#      description: "The {{ $labels.kubernetes_name }} denied to fix some problems as there are too many of them - this should be investigated ..."
#      summary: "The {{ $labels.kubernetes_name }} denied to fix some problems - should be investigated ..."

  - alert: OpenstackVcenterNannyConsistencyTooMuchToFixByHand
    expr: sum(vcenter_nanny_consistency_cinder_volume_in_use_without_some_attachments) > vcenter_nanny_consistency_cinder_volume_attachment_max_fix_count
    for: 30m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} found a lot of problems which will have to be fixed by hand - this should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} found lots of problems to manually fix - should be investigated ..."

  - alert: OpenstackVcenterNannyConsistencyNoAutoFix
    expr: vcenter_nanny_consistency_no_autofix > 0
    for: 5m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} found some problems which will have to be fixed by hand - this should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} found some problems to manually fix - should be investigated ..."

  # do it all very slow and smoothly averaged to not give false alerts on nanny restarts - alert when it grows over about 5% within 12 hours
  - alert: OpenstackVcenterNannyConsistencyInstanceVolumeZeroSize
    expr: (avg_over_time(vcenter_nanny_consistency_vcenter_volume_zero_size[3h]) + 5)/(avg_over_time(vcenter_nanny_consistency_vcenter_volume_zero_size[3h] offset 12h) + 5) > 1.20
    for: 3h
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered an increase of the number of zero size disks, please keep in mind that this actually happened around 6 hours ago due to alert smooting - should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} discovered an increase of the number of zero size disks - should be investigated ..."

  # do it all very slow and smoothly averaged to not give false alerts on nanny restarts - alert when it grows over about 5% within 12 hours
  - alert: OpenstackVcenterNannyConsistencyInstanceStatusGray
    expr: (avg_over_time(vcenter_nanny_consistency_vcenter_instance_status_gray[3h]) + 5)/(avg_over_time(vcenter_nanny_consistency_vcenter_instance_status_gray[3h] offset 12h) + 5) > 1.20
    for: 3h
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered an increase of the number of instances with gray status, please keep in mind that this actually happened around 6 hours ago due to alert smooting - should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} discovered an increase of the number of instances with gray status - should be investigated ..."

#  - alert: OpenstackVcenterNannyConsistencyVolumeExtraConfigWithoutBackingUuid
#    expr: vcenter_nanny_consistency_vcenter_extraconfig_backinguuid_missing > 0
#    for: 90m
#    labels:
#      context: nanny
#      service: nanny
#      severity: info
#      tier: os
#    annotations:
#      description: "The {{ $labels.kubernetes_name }} discovered a volume with a missing extraConfig entry for a backing uuid - should be investigated ..."

#  - alert: OpenstackVcenterNannyConsistencyVolumeBackingUuidWithoutExtraconfig
#    expr: vcenter_nanny_consistency_vcenter_backinguuid_extraconfig_missing > 0
#    for: 90m
#    labels:
#      context: nanny
#      service: nanny
#      severity: info
#      tier: os
#    annotations:
#      description: "The {{ $labels.kubernetes_name }} discovered a volume with a missing backing uuid for an extraConfig entry - should be investigated ..."

#  - alert: OpenstackVcenterNannyConsistencyVolumeBackingUuidMismatch
#    expr: vcenter_nanny_consistency_vcenter_volume_backing_uuid_mismatch > 0
#    for: 5m
#    labels:
#      context: nanny
#      service: nanny
#      severity: info
#      tier: os
#    annotations:
#      description: "The {{ $labels.kubernetes_name }} discovered a volume with a mismatch of the backing uuid - should be investigated ..."
#      summary: "The {{ $labels.kubernetes_name }} discovered a volume with a mismatch of the backing uuid - should be investigated ..."

#  - alert: OpenstackVcenterNannyConsistencyVolumeUuidMismatch
#    expr: vcenter_nanny_consistency_vcenter_volume_uuid_mismatch > 0
#    for: 5m
#    labels:
#      context: nanny
#      service: nanny
#      severity: info
#      tier: os
#    annotations:
#      description: "The {{ $labels.kubernetes_name }} discovered an instance with a instanceUuid to name mismatch - should be investigated ..."
#      summary: "The {{ $labels.kubernetes_name }} discovered an instance with a instanceUuid to name mismatch - should be investigated ..."

  - alert: OpenstackVcenterNannyConsistencyInstanceNameMismatch
    expr: vcenter_nanny_consistency_vcenter_instance_name_mismatch > 0
    for: 5m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered an instance with a instance name to config.name mismatch - should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} discovered an instance with a instance name to config.name mismatch - should be investigated ..."

  - alert: OpenstackVcenterNannyConsistencyBBNotInAggregate
    expr: vcenter_nanny_consistency_vcenter_bb_not_in_aggregate > 0
    for: 5m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
    annotations:
      description: "The {{ $labels.kubernetes_name }} discovered that not all vc hosts are properly configured in the vc-* aggregates - should be investigated ..."
      summary: "The {{ $labels.kubernetes_name }} discovered that not all vc hosts are properly configured in the vc-* aggregates - should be investigated ..."

  - alert: ESXiHostOverbooked
    expr: vm_balance_nanny_suggestion_bytes > 0
    for: 5m
    labels:
      service: compute
      severity: warning
      tier: vmware
      no_alert_on_absence: "true"
      playbook: docs/devops/alert/vcenter/#test_esxi_hs_4
    annotations:
      description: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host => suggestion: VMotion Big_vm {{ $labels.big_vm_name}} from source {{ $labels.source_node}} to target {{ $labels.target_node}} "
      summary: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host"

  - alert: ESXiHostOverbookedAutomation
    expr: vm_balance_vmotion_status > 0
    for: 5m
    labels:
      context: nanny-automation
      service: nanny
      severity: info
      tier: vmware
      no_alert_on_absence: "true"
      playbook: docs/devops/alert/vcenter/#test_esxi_hs_4
    annotations:
      description: "ESXi host {{ $labels.kubernetes_name }} VMotion Big_vm {{ $labels.big_vm_name}} from source {{ $labels.source_node}} to target {{ $labels.target_node}} successful "
      summary: "Overbooked ESXi host cleared successfully for below host by automatic nannies"

  - alert: ESXiHostOverbookedNoSuitableVM
    expr: vm_balance_no_suitable_vm > 0
    for: 5m
    labels:
      service: compute
      severity: warning
      tier: vmware
      no_alert_on_absence: "true"
      playbook: docs/devops/alert/vcenter/#test_esxi_hs_4
    annotations:
      description: "ESXi host {{ $labels.kubernetes_name }} and source {{ $labels.source_node}} is overbooked dont find any large VM till 512GB for movement after considering considering vm_readiness and host_contention metrics"
      summary: "ESXi host {{ $labels.kubernetes_name }} and source {{ $labels.source_node}} is overbooked"

  - alert: ESXiHostOverbookedAutomationError
    expr: vm_balance_error_count > 0
    for: 5m
    labels:
      context: nanny
      service: nanny
      severity: info
      tier: os
      no_alert_on_absence: "true"
    annotations:
      description: "The vcenter {{ $labels.kubernetes_name }} with Overbooked node Automation found error {{ $labels.error_type }} should be investigated"
      summary: "The vcenter {{ $labels.kubernetes_name }} with Overbooked node Automation found error {{ $labels.error_type }} should be investigated"

  - alert: ESXiHostOverbookedManualwithNannySuggestion
    expr: vm_balance_nanny_manual_suggestion_bytes > 0
    for: 10m
    labels:
      service: compute
      severity: warning
      tier: vmware
      no_alert_on_absence: "true"
      playbook: docs/devops/alert/vcenter/#test_esxi_hs_4
    annotations:
      description: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host => suggestion: VMotion Big_vm {{ $labels.big_vm_name}} from source {{ $labels.source_node}} to target {{ $labels.target_node}} "
      summary: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host"

  - alert: ESXiHostOverbookedManualwithoutNannySuggestion
    expr: vm_balance_too_full_building_block > 0
    for: 10m
    labels:
      service: compute
      severity: warning
      tier: vmware
      no_alert_on_absence: "true"
      playbook: docs/devops/alert/vcenter/#test_esxi_hs_4
    annotations:
      description: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host overbooked is {{ $labels.consolidated_needed }} => No suggestion from nanny and consolidated_needed in BB:"
      summary: "ESXi host {{ $labels.kubernetes_name }} physical memory not enough to host all VMs >= 256GB which are placed on the host"

  - alert: ESXiHostOverutilizedConfiguredMemoryGB
    expr: (vm_balance_nanny_host_size_consume_all_vm_bytes - vm_balance_nanny_host_size_bytes)/1024/1024/1024 > 400
    for: 10m
    labels:
      service: compute
      severity: info
      tier: vmware
    annotations:
      description: "ESXi host {{ $labels.nodename }} overutilized by more then 400 GB on host configured memory size"

  - alert: ESXiClusterOverutilizedConfiguredMemoryPercentage
    expr: (vm_balance_building_block_consume_all_vm_bytes * 100 / (vm_balance_building_block_total_size_bytes )) > 95
    for: 10m
    labels:
      service: compute
      severity: info
      tier: vmware
    annotations:
      description: "BuildingBlock_BB{{ $labels.Building_block }} more then 95% utilized on configured memory"

  - alert: PredictHostMemoryBallooning
    expr: (vm_balance_nanny_host_size_consume_all_vm_bytes{region=~"eu-de-1|eu-de-2"} - vm_balance_nanny_host_size_bytes{region=~"eu-de-1|eu-de-2"})/1024/1024/1024 > 200 and abs(vm_balance_nanny_host_size_consume_big_vm_bytes{region=~"eu-de-1|eu-de-2"} - vm_balance_nanny_host_size_bytes{region=~"eu-de-1|eu-de-2"})/1024/1024/1024 < 200
    for: 10m
    labels:
      service: compute
      severity: info
      tier: vmware
    annotations:
      description: "ESXi host {{ $labels.nodename }} having chances of host memory Ballooning on configured memory"
