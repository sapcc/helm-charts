vcenter_nanny:
  enabled: false
  docker_repo: DEFINED-IN-REGION-CHART
  image_version: DEFINED-IN-REGION-CHART
  debug: true
  # dry run mode, i.e. just pretend to cleanup the entries
  dry_run: true
  # power off machines discovered as orphaned
  power_off: false
  # unregister machines discovered as orphaned
  unregister: false
  # delete machines discovered as orphaned
  delete: false
  # detach discovered ghost volumes
  detach_ghost_volumes: false
  # detach discovered ghost ports
  detach_ghost_ports: false
  # how many ghost volumes or ports to detach at maximum - if more: deny to delete any
  detach_ghost_limit: 3
  # enable or disable volume attachment consistency check
  vol_check: false
  # size of vms considered a bigvm for drs disablement in gb
  bigvm_size: 1024
  # size of vms considered a bigvm for setting memory shares to high in gb
  bigvm_shares_action_size: 255
  # run the db cleanup every n minutes
  interval: 720
  # really delete entities after how many iterations
  iterations: 14

# these are used by the vcenter-nanny, but set during deployment via helm --set-string
from_cinder:
  global:
    dbPassword: "will_be_set_by_helm_deployment"
from_nova:
  global:
    dbPassword: "will_be_set_by_helm_deployment"
current_region: "will_be_set_by_helm_deployment"

vcenter_nanny_consistency:
  enabled: false
  # this one we use from the vcenter nanny entry above as we use the same docker repo
  # docker_repo: DEFINED-IN-REGION-CHART
  consistency_image_version: DEFINED-IN-REGION-CHART
  debug: true
  # dry run mode, i.e. just pretend to cleanup the entries
  dry_run: true
  # run the db consistency check every minutes
  interval: 60
  # warn after how many iterations
  iterations: 3
  # maximum number of inconsistent volumes to fix automatically - otherwise fixing will be denied
  fix_limit: 25
  # if cells are enabled, put the VCENTER_CONSISTENCY_HOST here as a reference to the az with an own cell db - false if not relevant
  cell2_vc: false

# nova nanny
nova_nanny:
  image_version: DEFINED-IN-REGION-CHART
  enabled: false
  # run the pod with an infinite sleep loop for debugging
  debug: false
  # run the nanny every n minutes
  interval: 15
  # the quota sync part is hard disabled in the deployment for now as we do not need it anymore and to avoid accidental enablement
  quota_sync:
    enabled: false
  db_purge:
    enabled: false
    # dry run mode, i.e. just pretend to purge the entries
    dry_run: true
    # purge instance entries older than n days
    older_than: 14
    # purge at max n entries in one run
    max_number: 50
  consistency:
    enabled: false
    # dry run mode, i.e. just check for consistency without fixing it
    dry_run: true
    # detect problems in the instance mappings we saw in queens
    queens_instance_mapping_enabled: true
    # dry run mode, i.e. just check for consistency without fixing it
    queens_instance_mapping_dry_run: true
    # purge deleted block_device_mappings and reservations older than n days
    older_than: 21
    # keep at max n instance fault entries per instance
    max_instance_faults: 10
    # how many inconsistencies to fix at max - otherwise fixing will be denied
    fix_limit: 25
  db_cleanup:
    enabled: false
    # dry run mode, i.e. just pretend to cleanup the entries
    dry_run: true
    # run the db cleanup every n minutes
    interval: 720
    # really delete entities after how many iterations
    iterations: 14
  sync_neutron_cache:
    enabled: false
    # dry run mode, i.e. just pretend to cleanup the entries
    dry_run: true
    # run the db cleanup every n minutes
    interval: 360
    # sync the neutron cache for bare metal nodes as well?
    baremetal: false
  cell2:
    # this should be true if we have a second cell defined in this region
    enabled: false

# cinder nanny
cinder_nanny:
  image_version: DEFINED-IN-REGION-CHART
  enabled: false
  # run the pod with an infinite sleep loop for debugging
  debug: false
  # run the nanny every n minutes
  interval: 60
  quota_sync:
    enabled: false
  db_purge:
    enabled: false
    # purge deleted cinder entities older than n days
    older_than: 14
  consistency:
    enabled: false
    # dry run mode, i.e. just check for consistency without fixing it
    dry_run: true
    # how many inconsistencies to fix at max - otherwise fixing will be denied
    fix_limit: 25
  db_cleanup:
    enabled: false
    # dry run mode, i.e. just pretend to cleanup the entries
    dry_run: true
    # run the db cleanup every n minutes
    interval: 720
    # really delete entities after how many iterations
    iterations: 14

# glance nanny
glance_nanny:
  image_version: DEFINED-IN-REGION-CHART
  enabled: false
  # debug mode - no script run in the containers, just a sleep loop
  debug: true
  # run the nanny every n minutes
  interval: 60
  db_purge:
    enabled: true
    # purge deleted db entries older than n days
    older_than: 14
    # delete at max number of entries in one run
    max_number: 50

# neutron nanny
neutron_nanny:
  image_version: DEFINED-IN-REGION-CHART
  enabled: false
  # run the pod with an infinite sleep loop for debugging
  debug: false
  cleanup_pending_lb:
    enabled: false
    # dry run mode, i.e. just pretend to cleanup the entries
    dry_run: true
    # run the db cleanup every n minutes
    interval: 60
    # really delete entities after how many iterations
    iterations: 3
  # is mariadb enabled in manila deployment or not
  mariadb:
    enabled: false

vm_balance_nanny:
  enabled: false
  debug: true
  image_version: '20200721220608'
  interval: 120

netapp_balance_nanny:
  enabled: true
  debug: false
  # dry run mode - do not do anything potentially harmful
  dry_run: true
  image_version: '20200715141151'
  # how many minutes to wait until the next nanny run
  interval: 180
  # max usage of an aggregate in percent to still move volumes to it
  minthreshold: 69
  # min usage of an aggregate in percent to still move volumes away from it
  maxthreshold: 73
  # how many percent to go below maxthreshold by balancing volumes away
  maxthresholdhysteresis: 2
  # legacy - will go soon
  volumeminsize: 250000
  volumemaxsize: 2000000
  # maximum number of volumes to move for flexvol and aggr balancing (separately for both)
  maxmovevms: 50
  # min usage of a flexvol in gb to move volumes away from it
  # about 90% of the typical flexvol size of 5tb to avoid it autoextending early on
  flexvolsizelimit: 4600
  # min (>=) size of a volume in gb to move away from a flexvol for balancing
  lunminsizeflexvol: 25
  # max (<) size of a volume in gb to move away from a flexvol for balancing
  lunmaxsizeflexvol: 1000
  # min (>=) size of a volume in gb to move away from an aggregate for balancing
  # best = lunmaxsizeflexvol to avoid overlapping balancing between flexvol and aggr
  lunminsizeaggr: 1000
  # max (<=) size of a volume in gb to move away from an aggregate for balancing
  lunmaxsizeaggr: 2500

# manila nanny
manila_nanny:
  image_version: 'train-20200708132903'
  image_netapp_version: 'train-20200611151333'
  enabled: false
  # debug mode - no script run in the containers, just a sleep loop
  debug: false
  # run the nanny every n seconds
  interval: 3600
  db_purge:
    enabled: false
    dry_run: true
    # purge deleted db entries older than n days
    older_than: 14
  consistency:
    enabled: false
    dry_run: true
  quota_sync:
    enabled: false
    dry_run: true
    resources:
      requests:
        memory: "64Mi"
        cpu: "300m"
  share_sync:
    enabled: false
    dry_run: true
    interval: 3600
    task_share_size: true
    task_share_size_dry_run: false
    task_missing_volume: true
    task_missing_volume_dry_run: true
    task_offline_volume: true
    task_offline_volume_dry_run: true
    task_orphan_volume: true
    task_orphan_volume_dry_run: true
    http_port: 8002
    prometheus_port: 9502
  share_server:
    enabled: false
    listen_port: 8000
    prometheus_port: 9500
  snapshot:
    enabled: false
    listen_port: 8001
    prometheus_port: 9501
  db_cleanup:
    enabled: false
    dry_run: true
  netapp:
    enabled: true
    interval: 240
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
  # default pod resources of nanny containers, can be overridden by individual nanny
  resources:
    requests:
      memory: "64Mi"
      cpu: "50m"
  # is mariadb enabled in manila deployment or not
  mariadb:
    enabled: false

# Deploy Nanny Prometheus alerts.
alerts:
  enabled: true
  # Name of the Prometheus to which the alerts should be assigned to.
  prometheus: openstack

# currently used by glance nanny only
sentry:
  enabled: true
