groups:
- name: octavia.alerts
  rules:
  - alert: OpenstackOctaviaMonitorAgentHeartbeat
    expr: max(octavia_monitor_agents_heartbeat_seconds) > 120
    for: 10m
    labels:
      context: Agent Heartbeat
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: 'Agent Heartbeat is above 75secs in {{ $labels.octavia_host }}'
      playbook: docs/support/playbook/octavia/agent_heartbeat.html
    annotations:
      description: Agent Heartbeat is above 75secs in {{ $labels.octavia_host }}
      summary: Openstack Octavia Metric to monitor Agents Heartbeat
  - alert: OpenstackOctaviaWorkerConnectionError
    expr: rate(octavia_as3_failover_total{job="pods"}[5m]) > 0
    for: 5m
    labels:
      context: F5 Failover
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: 'Forced failover for AS3 target {{ $labels.app_kubernetes_io_name }} due to connection error'
    annotations:
      description: 'Forced failover for AS3 target {{ $labels.app_kubernetes_io_name }} due to connection error'
      summary: Openstack Octavia Metric to monitor connection errors and AS3 targets
  - alert: OpenstackOctaviaFailoverEvent
    expr: rate(octavia_status_failover_total[5m]) > 0
    for: 5m
    labels:
      context: F5 Failover
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: 'Failover detected for agent {{ $labels.app_kubernetes_io_name }}.'
    annotations:
      description: 'Failover detected for agent {{ $labels.app_kubernetes_io_name }}, AS3 target will be changed to active device.'
      summary: Openstack Octavia Metric to monitor F5 Worker Agent failover events
  - alert: OpenstackOctaviaLoadbalancerErrored
    expr: rate(octavia_loadbalancers_count_gauge{status="ERROR"}[5m]) > 0
    for: 10m
    labels:
      context: Loadbalancer go to ERROR
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: 'Load Balancers in {{ $labels.octavia_host }} gone to ERROR state'
    annotations:
      description: Loadbalancers in {{ $labels.octavia_host }} gone to ERROR state
      summary: Openstack Octavia Metric to monitor erroneous loadbalancers
  - alert: OpenstackOctaviaLoadbalancerPending
    expr: octavia_loadbalancers_count_gauge{status=~"PENDING.*"} > 0
    for: 30m
    labels:
      context: Loadbalancer stuck in PENDING state
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: 'Load Balancers in {{ $labels.octavia_host }} stuck in {{ $labels.status }} state'
    annotations:
      description: Load Balancers in {{ $labels.octavia_host }} stuck in {{ $labels.status }} state
      summary: Openstack Octavia Metric to monitor pending loadbalancers
  - alert: OpenstackOctaviaMonitorStuck
    expr: increase(octavia_as3_stuck_monitor_total[5m]) > 3
    for: 5m
    labels:
      context: Stuck Monitor discovered
      dashboard: octavia
      service: octavia
      severity: info
      tier: os
      meta: ''
    annotations:
      description: Stuck Monitor workaround applied for {{ $labels.app_kubernetes_io_name }}, please consider device reload
      summary: Stuck Monitor workaround in action
