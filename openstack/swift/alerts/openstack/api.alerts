groups:
- name: openstack-swift-api.alerts
  rules:
  - alert: OpenstackSwiftFirstByteTiming
    expr: min(sum(irate(swift_proxy_firstbyte_bucket{le="1",type!="container",status=~"20[0-9]"}[5m])) by (os_cluster, type, instance) / sum(irate(swift_proxy_firstbyte_bucket{le="+Inf",type!="container",status=~"20[0-9]"}[5m])) by (os_cluster, type, instance)) by (os_cluster, type, instance) < 0.5
    for: 15m
    labels:
      context: firtsbytetiming
      dashboard: swift-proxy?var-cluster={{ $labels.os_cluster }}&var-proxy={{ $labels.instance }}
      service: swift
      severity: info
      tier: os
      meta: first byte timing for {{ $labels.type }} in {{ $labels.os_cluster }} increased
    annotations:
      description: This alert indicates the latency in token validation.
      summary: first byte timing for {{ $labels.type }} in {{ $labels.os_cluster }}
        increased

  - alert: OpenstackSwiftServiceUnavailable
    expr: sum(irate(swift_proxy_sum{policy="all",status="503"}[5m])) BY (os_cluster) > 100
    for: 5m
    labels:
      context: serviceunavailable
      dashboard: swift-proxy?var-cluster={{ $labels.os_cluster }}
      service: swift
      severity: warning
      tier: os
      meta: 503 responses from Swift in {{ $labels.os_cluster }} (check Keystone availability)
    annotations:
      description: Swift is responding with 503 in {{ $labels.os_cluster }}. Usually the root cause is a broken
        token validation
      summary: swift-service-unavailable

  - alert: OpenstackSwiftHealthCheck
    expr: avg(swift_health_statsd_exit_code) BY (region) > 0.2 or avg(swift_recon_task_exit_code) BY (region) > 0.2 or avg(swift_dispersion_task_exit_code) BY (region) > 0.2 or absent(swift_dispersion_task_exit_code)
    for: 5m
    labels:
      context: health
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: some health checks for Swift are failing
    annotations:
      description: Swift health check failures. Run kubectl log (swift-proxy-... / swift-health-exporter-...) collector to get details
      summary: swift-health-check

  # TODO: remove this when the new exporter gets rolled out
  - alert: OpenstackSwiftMismatchedRingsOld
    expr: (swift_cluster_md5_ring_not_matched - swift_cluster_md5_ring_errors) > 0
    for: 15m
    labels:
      context: mismatchedrings
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Rings are not equal on all Swift nodes
    annotations:
      description: Rings are not equal on all nodes
      summary: swift-mismatched-rings

  - alert: OpenstackSwiftMismatchedRings
    expr: (swift_cluster_md5_not_matched{kind="ring"} - swift_cluster_md5_errors{kind="ring"}) > 0
    for: 15m
    labels:
      context: mismatchedrings
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Rings are not equal on all Swift nodes
    annotations:
      description: Rings are not equal on all nodes
      summary: swift-mismatched-rings

  # TODO: remove this when the new exporter gets rolled out
  - alert: OpenstackSwiftMismatchedConfigOld
    expr: (swift_cluster_md5_swiftconf_not_matched - swift_cluster_md5_swiftconf_errors) > 0
    for: 15m
    labels:
      context: mismatchedconf
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Configuration is not equal on all Swift nodes
    annotations:
      description: Configuration is not equal on all nodes
      summary: swift-mismatched-config

  - alert: OpenstackSwiftMismatchedConfig
    expr: (swift_cluster_md5_not_matched{kind="swiftconf"} - swift_cluster_md5_errors{kind="swiftconf"}) > 0
    for: 15m
    labels:
      context: mismatchedconf
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Configuration is not equal on all Swift nodes
    annotations:
      description: Configuration is not equal on all nodes
      summary: swift-mismatched-config

  # TODO: remove this when the new exporter gets rolled out
  - alert: OpenstackSwiftNodeErrorOld
    expr: max(swift_cluster_md5_ring_errors) by (region) > 0
    for: 15m
    labels:
      context: nodeerror
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: '{{ $value }} Swift nodes not reachable'
    annotations:
      description: Swift node error. {{ $value }} Node(s) not reachable.
      summary: swift-node-error

  - alert: OpenstackSwiftNodeError
    expr: max(swift_cluster_md5_errors{kind="ring"}) by (region) > 0
    for: 15m
    labels:
      context: nodeerror
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: '{{ $value }} Swift nodes not reachable'
    annotations:
      description: Swift node error. {{ $value }} Node(s) not reachable.
      summary: swift-node-error

  - alert: OpenstackSwiftDisksUnmounted
    expr: max(swift_cluster_drives_unmounted) BY (storage_ip) > 0
    for: 3m
    labels:
      context: drivesunmounted
      dashboard: swift-overview
      service: swift
      severity: info
      tier: os
      meta: '{{ $value }} Swift drives not mounted'
    annotations:
      description: Swift drives not mounted. Run swift-recon --unmounted to get details.
        Also check #alert-metal-warning for disk failures. They should get acknowledged there.
      summary: swift disks unmounted on the server IP {{ $labels.storage_ip }}

  - alert: OpenstackSwiftDiskFailures
    expr: max(swift_cluster_drives_audit_errors) BY (storage_ip) > 0
    for: 3m
    labels:
      context: drivefailures
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: metal
      meta: '{{ $value }} Swift drive failures'
      playbook: 'docs/support/playbook/swift/disk_failures.html'
    annotations:
      description: Swift drive failures. Run swift-recon --driveaudit to get details
      summary: swift disk failures on the server IP {{ $labels.storage_ip }}

  - alert: OpenstackSwiftAsyncPendings
    expr: sum(increase(swift_object_server_async_pendings[5m])) > 5000
    for: 15m
    labels:
      context: asyncpendings
      dashboard: swift-overview
      service: swift
      severity: info
      tier: os
      meta: Async Pendings indicate an overload scenario or faulty device(s)
    annotations:
      description: Async Pendings indicate an overload scenario or faulty device(s)
      summary: swift object server async pendings reached 5000

  - alert: OpenstackSwiftRateLimit1000
    expr: sum(increase(swift_proxy_count{status="498"}[5m])) BY (os_cluster) > 1000
    for: 3m
    labels:
      context: ratelimit
      dashboard: swift-overview
      service: swift
      severity: info
      tier: os
      meta: Some accounts/containers are being rate-limited in {{ $labels.os_cluster }}
    annotations:
      description: Check kibana for the causing account and container
      summary: swift rate limit hit in {{ $labels.os_cluster }}

  - alert: OpenstackSwiftRateLimit5000
    expr: sum(increase(swift_proxy_count{status="498"}[5m])) BY (os_cluster) > 5000
    for: 3m
    labels:
      context: ratelimit
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Some accounts/containers are being rate-limited in {{ $labels.os_cluster }}
    annotations:
      description: Check kibana for the causing account and container
      summary: swift rate limit hit in {{ $labels.os_cluster }}

  - alert: OpenstackSwiftUsedSpace
    expr: max(predict_linear(global:swift_cluster_storage_used_percent_average[1w], 60 * 60 * 24 * 30)) > 0.8
    for: 1d
    labels:
      context: usedcapacity
      dashboard: swift-capacity-global?var-region={{ $labels.region }}
      service: swift
      severity: info
      tier: os
      meta: Swift storage usage will reach 80% in 30 days. Order hardware now!
    annotations:
      description: Swift storage usage will reach 80% in 30 days. Order hardware now!
      summary: Swift storage expected to be full soon

  - alert: OpenstackSwiftDriveAutopilotConsistencyCheck
    expr: irate(swift_drive_autopilot_events{type="consistency-check"}[5m]) < 0.02
    for: 10m
    labels:
      context: autopilot
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: '{{ $labels.kubernetes_pod_name }} not performing consistency checks on schedule'
    annotations:
      description: Autopilot {{ $labels.kubernetes_pod_name }} does not perform its
        consistency checks on schedule. Please check if it's having a bad time.
      summary: No consistency checks performed

  - alert: OpenstackSwiftAccountReplicationCheck
    expr: max(swift_cluster_accounts_replication_age) by (storage_ip) > 3*3600
    labels:
      context: accountrepl
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Account replication getting stuck on {{ $labels.storage_ip }}
    annotations:
      description: "The last successful account replication age on {{ $labels.storage_ip }}
        is older than 3 hours. Check the affected node. A restart of the replicator pod
        might be necessary."
      summary: Account replication on {{ $labels.storage_ip }} older than 3 hours

  - alert: OpenstackSwiftContainerReplicationCheck
    expr: max(swift_cluster_containers_replication_age) by (storage_ip) > 3*3600
    labels:
      context: containerrepl
      dashboard: swift-overview
      service: swift
      severity: warning
      tier: os
      meta: Container replication getting stuck on {{ $labels.storage_ip }}
    annotations:
      description: "The last successful container replication age on {{ $labels.storage_ip }}
        is older than 3 hours. Check the affected node. A restart of the replicator pod might
        be necessary."
      summary: Container replication on {{ $labels.storage_ip }} older than 3 hours

  - alert: OpenstackSwiftApiDown
    expr: blackbox_api_status_gauge{check=~"swift"} == 1
    for: 20m
    labels:
      severity: critical
      tier: os
      service: '{{ $labels.service }}'
      context: '{{ $labels.service }}'
      dashboard: ccloud-health-blackbox-details
      meta: '{{ $labels.check }} API is down. See Sentry for details.'
      sentry: 'blackbox/?query=test_{{ $labels.check }}'
      playbook: 'docs/devops/alert/swift/#{{ $labels.check }}'
    annotations:
      description: '{{ $labels.check }} API is down for 20 min. See Sentry for details.'
      summary: '{{ $labels.check }} API down'

  - alert: OpenstackSwiftApiFlapping
    expr: changes(blackbox_api_status_gauge{check=~"swift"}[30m]) > 8
    labels:
      severity: warning
      tier: os
      service: '{{ $labels.service }}'
      context: '{{ $labels.service }}'
      dashboard: ccloud-health-blackbox-details
      meta: '{{ $labels.check }} API is flapping'
      sentry: 'blackbox/?query=test_{{ $labels.check }}'
      playbook: 'docs/devops/alert/swift/#{{ $labels.check }}'
    annotations:
      description: '{{ $labels.check }} API is flapping for 30 minutes.'
      summary: '{{ $labels.check }} API flapping'

  - alert: OpenstackSwiftCanaryDown
    expr: blackbox_canary_status_gauge{service=~"swift"} == 1
    for: 1h
    labels:
      severity: warning
      tier: os
      service: '{{ $labels.service }}'
      context: '{{ $labels.service }}'
      dashboard: ccloud-health-canary-details
      meta: 'Canary {{ $labels.service }} {{ $labels.check }} is down for 1 hour. See Sentry for details'
      sentry: 'blackbox/?query=test_{{ $labels.check }}'
      playbook: 'docs/devops/alert/{{ $labels.service }}'
    annotations:
      description: 'Canary {{ $labels.service }} {{ $labels.check }} is down for 1 hour. See Sentry for details'
      summary: 'Canary {{ $labels.service }} {{ $labels.check }} is down'

  - alert: OpenstackSwiftCanaryTimeout
    expr: blackbox_canary_status_gauge{service=~"swift"} == 0.5
    for: 1h
    labels:
      severity: info
      tier: os
      service: '{{ $labels.service }}'
      context: '{{ $labels.service }}'
      dashboard: ccloud-health-canary-details
      meta: 'Canary {{ $labels.service }} {{ $labels.check }} is timing out for 1 hour. See Sentry for details'
      sentry: 'blackbox/?query=test_{{ $labels.check }}'
      playbook: 'docs/devops/alert/{{ $labels.service }}'
    annotations:
      description: 'Canary {{ $labels.service }} {{ $labels.check }} is timing out for 1 hour. See Sentry for details'
      summary: 'Canary {{ $labels.service }} {{ $labels.check }} is timing out'

  - alert: OpenstackSwiftCanaryFlapping
    expr: changes(blackbox_canary_status_gauge{service=~"swift"}[2h]) > 8
    labels:
      severity: warning
      tier: os
      service: '{{ $labels.service }}'
      context: '{{ $labels.service }}'
      dashboard: ccloud-health-canary-details
      meta: 'Canary {{ $labels.service }} {{ $labels.check }} is flapping for 2 hours. See Sentry for details'
      sentry: 'blackbox/?query=test_{{ $labels.check }}'
      playbook: 'docs/devops/alert/{{ $labels.service }}'
    annotations:
      description: 'Canary {{ $labels.service }} {{ $labels.check }} is flapping for 2 hours. See Sentry for details'
      summary: 'Canary {{ $labels.service }} {{ $labels.check }} is flapping'
