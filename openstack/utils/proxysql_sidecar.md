# ProxySQL side-car snippets

All macros are not producing any output, when the value
`proxysql.mode` is set to a falsy value.
There are currently two modes, one is `"unix_socket"` and `"host_alias"`.
Unless either mode is set explicitly the macros should not create any change
in the deployment.
So, when all changes listed here are done, the resulting diff should be empty.

The changes needed are:
- adding a Secret for the ProxySQL side-car container configuration, and
- modifying each and every Pod-spec in Jobs, Deployments, StatefulSets, etc... (for pods that access databases)

The two modes differ in the following way:
- `unix_socket` connects to the side-car container via a unix socket file on a shared volume.
   This should be the fastest, but has the disadvantage that works only with the side-car present, as the url
   is referring to said file.
- `host_alias`  uses a host alias for the short hostname (ie. `<app>-mariadb`) to redirect the application to the localhost
  where the side-car container is listening on the default port. But the very same url also works without a side-car
  container, if the host-alias is not set. So we do not have to annotate all pods, we just have to make sure,
  that the annotations are consistent for a single pod.

## Configuration

The configuration for the ProxySQL process needs to be stored in a Secret as
generated by the macro `proxysql_secret`.
If only one mariadb instance with one schema and user is used, it suffices to create a yaml file with
```
{{ include "proxysql_secret" . }}
```
When multiple databases / schemas or users are used, the mariadb dependency needs to be updated at least to version 0.4.0,
and the users and databases need to be defined in the respective `databases` and `users` fields.
There can be multiple databases/schema on the same host, but a user always needs to map directly to one host & database.

To enable monitoring for proxysql, the `mariadb.users` field needs to contain a user with the key `proxysql_monitor`.
If there are multiple mariadb instances, each of them needs to have the field entered and all of them have to have the same
values.

Following the pattern of all macros in utils, the values are coming from the context
of the main chart, despite being included as "utils" chart.
The values and their defaults are listed here, as we cannot set them in the values.yaml from the utils chart.
```
proxysql:
  mode: "unix_socket"         # You need to opt-in explicitly by selecting a mode (unix_socket or host_alias).
  image: "proxysql/proxysql"  # Directly from the vendor (via keppel dockerhub mirror)
  imageTag: "2.4.1-debian"    # Most current at the time of writing this
  restapi_port: 6070          # On which port to expose the rest-api (and prometheus metrics)
  prometheus_memory_metrics_interval: 61 # See: https://proxysql.com/documentation/global-variables/admin-variables/#admin-prometheus_memory_metrics_interval
  max_connections_per_proc: (max_pool_size + max_overflow)  # The maximum number of connections proxysql will open to a mariadb host
                              # per process. With the default assumption being one process per pod.
                              # The settings are modelled after the sqlalchemy pool, which is a per process pool
```

### Pod-Changes

*Attention*: Jobs need special care, as all containers in a pod need to terminate for the job to finish.

First lets have a look at the standard case:

To add the ProxySQL side-car to a pod you need to add at least three parts:
1. The container itself is rendered by the macro `utils.proxysql.container`,
and can be placed under the containers. It takes an optional integer specifying the number of processes (default 1)
as a multiplier for `proxysql.max_connections_per_proc` resulting in the same number of maximum connections per pod as the
per process setting `max_connections` of the sqlalchemy pool.
2. The volume with the unix socket is defined in `utils.proxysql.volumes`, and goes into the "volumes" section of the spec
3. To tie everything together, the volume needs to be mounted in each container accessing ProxySQL. This is done with the macro `utils.proxysql.volume_mount`. This is only necessary for the mode `unix_socket`.
4. For the host-alias to work, we need to annotate the pod-spec with `utils.proxysql.job_pod_settings`, so it sets the hostAlias field
   of the spec for the pod, redirecting any dns lookups of the short-hostname to localhost. This only necessary for the mode `host_alias`

If `volumeMounts` is the last section of the container accessing the database,
the snippets `utils.proxysql.container` and `utils.proxysql.container` can be placed directly together like this:
```
       containers:
         - name: ...
           ...
           volumeMounts:
             ...
+          {{- include "utils.proxysql.volume_mount" . | indent 12 }}
+        {{- tuple . .Values.rpc_workers | include "utils.proxysql.container" | indent 8 }}
      ...
      volumes:
        ...
+        {{- include "utils.proxysql.volumes" . | indent 8 }}
```

Now to the Job: As mentioned before, in a Job all containers in a Pod need to terminate
for the the Job to finish. This means, the ProxySQL process needs to get signalled to terminate,
when the "main" container finishes.

For that purpose, the macro `utils.proxysql.job_pod_settings` to be added to the pod spec sets the user-id
of all the jobs to the same, and enables a shared process namespace, so that a user in another container can
signal the ProxySQL process to terminate.
For a shell script, this can be done via the `utils.proxysql.proxysql_signal_stop_script` macro.
The placement depends on the restart policy of the "main" container.
If the restart-policy is the default "OnFailure", we need to stop ProxySQL process only when the
script finishes normally. I.e by placing the macro `utils.proxysql.proxysql_signal_stop_script` at the end of the script, and any other place the script exits normally.
If the restart-policy is "Never", then we need to stop the ProxySQL process at every exit path.

That can be done by placing the following snippet before any possible exit:
```
trap "{{ include "utils.proxysql.proxysql_signal_stop_script" . }}" EXIT
```

Following all those steps should give you an app running over a ProxySQL-sidecar container.

