# vim: set ft=yaml:

groups:
- name: openstack-limes.alerts
  rules:

  # This alert is used for detecting unusually high API load before it becomes
  # a problem. We don't care about the collector; it self-regulates by slowing
  # down when its connection pool is exhausted.
  - alert: OpenstackLimesDBConnectionPoolNearlyFull
    expr: avg_over_time(go_sql_stats_connections_in_use{db_name="limes",kubernetes_pod_name!~"limes-collect-.*"}[1h]) > 8
    for: 5m
    labels:
      context: dbconnpool
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
      meta: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
    annotations:
      summary: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
      description: |
        The DB connection pool on pod {{ $labels.kubernetes_pod_name }} is filling up. It can
        go up to 16 connections, but during regular operations we should not go
        over 3-5 connections to retain some budget for request spikes. Going
        high on connections for a long time indicates that the pod might be
        starved for CPU time, so try checking the CPU throttling metrics.

  - alert: OpenstackLimesHttpErrors
    expr: sum(increase(http_requests_total{kubernetes_namespace="limes",code=~"5.*"}[1h])) by (kubernetes_name) > 0
    for: 5m
    labels:
      context: api
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
    annotations:
      description: "{{ $labels.kubernetes_name }} is producing HTTP responses with 5xx status codes."
      summary: Server errors on {{ $labels.kubernetes_name }}

  - alert: OpenstackLimesNotScraping
    expr: absent(rate(limes_resource_scrapes{task_outcome="success"}[30m]) > 0)
    for: 5m
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: There have been no successful scrapes in the last hour,
        so limes-collect-ccloud is probably dead.
      summary: Limes is not scraping

  - alert: OpenstackLimesFailedCapacityScrapes
    expr: sum(increase(limes_capacity_scrapes{task_outcome="failure"}[5m])) BY (capacitor_id) > 0
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: Limes cannot scrape capacity from {{ title $labels.capacitor_id }}
        for more than an hour.
        The `kubectl logs` for limes-collect-ccloud contain additional info.
      summary: Limes cannot scrape capacity {{ title $labels.capacitor }}

  - alert: OpenstackLimesMissingCapacity
    # This excludes:
    # - (explicitly) NoQuota resources
    # - (implicitly) resources that technically exist, but legitimately do not have capacity
    #   (e.g. HANA flavors for hypervisor types that do not exist in the region)
    expr: (global:limes_consolidated_cluster_capacity{full_resource!="sharev2/snapmirror_capacity"} == 0) and (max by (full_resource) (global:limes_consolidated_domain_usage) > 0)
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: info
      meta: 'no capacity for {{ $labels.full_resource }}'
      support_group: containers
      tier: os
    annotations:
      description: Limes reports no capacity for {{ $labels.full_resource }} even though there is assigned domain quota and/or usage.
        This usually happens because the backend service reported weirdly-shaped data to Limes' capacity scanner.
        The `kubectl logs` for limes-collect-ccloud may contain additional info.
        If not, running `limes test-scan-capacity` on the respective region and capacitor may provide additional insight.
      summary: Limes reports zero capacity for {{ $labels.full_resource }}

  - alert: OpenstackLimesFailedScrapes
    expr: sum(increase(limes_resource_scrapes{task_outcome="failure"}[5m])) BY (service_type, service_name, region) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: info
      support_group: '{{ if eq $labels.service_name "cinder" "nova" "manila" -}} compute-storage-api {{- else if eq $labels.service_name "designate" "neutron" "octavia" -}} network-api {{- else if eq $labels.service_name "swift" -}} storage {{- else -}} containers {{- end -}}'
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape data from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesFailingScrapes
    expr: max(limes_failing_scrapes_gauge) BY (service_name, region) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      support_group: '{{ if eq $labels.service_name "cinder" "nova" "manila" -}} compute-storage-api {{- else if eq $labels.service_name "designate" "neutron" "octavia" -}} network-api {{- else if eq $labels.service_name "swift" -}} storage {{- else -}} containers {{- end -}}'
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape data for projects from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesFailedRateScrapes
    expr: sum(increase(limes_rate_scrapes{task_outcome="failure"}[5m])) BY (service_type, service_name, region) > 0
    for: 1h
    labels:
      context: failedratescrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: info
      support_group: '{{ if eq $labels.service_name "cronus" -}} email {{- else -}} containers {{- end -}}'
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape rate data from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/rate-scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesFailingRateScrapes
    expr: max(limes_failing_rate_scrapes_gauge) BY (service_name, region) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      support_group: '{{ if eq $labels.service_name "cronus" -}} email {{- else -}} containers {{- end -}}'
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape rate data for projects from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesMissingSwiftMetrics
    expr: (limes_project_usage{service="object-store",resource="capacity"} > 0) unless on(project_id) (limes_plugin_metrics_ok{service="object-store"} == 1)
    for: 5m
    labels:
      context: swiftmetrics
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
      meta: "project {{ $labels.project_id }}"
    annotations:
      description: The project {{ $labels.domain }}/{{ $labels.project }} (UUID {{ $labels.project_id }})
        has Swift containers, but Limes does not submit Swift usage metrics to Maia.
        Check the `kubectl logs` for limes-collect-ccloud for additional info.
      summary: Limes cannot submit Swift usage metrics

  - alert: OpenstackLimesFailedKeystoneSyncs
    expr: sum(increase(limes_keystone_syncs{task_outcome="failure"}[5m])) > 0 or sum(increase(limes_keystone_syncs{task_outcome="success"}[5m])) == 0
    for: 30m
    labels:
      context: keystonesync
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: Limes cannot sync domains and projects from Keystone.
        Check the `kubectl logs` for limes-collect-ccloud for additional info.
      summary: Limes cannot sync from Keystone.

  - alert: OpenstackLimesAuditEventPublishFailing
    # Usually, you would check increase() here, but audit events *can* be quite
    # rare, so we alert if there are any failed audit events at all. To clear this alert,
    # delete the respective
    expr: max by (pod) (limes_failed_auditevent_publish > 0)
    for: 1h
    labels:
      context: auditeventpublish
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
      meta: '{{ $labels.pod }}'
    annotations:
      summary: "{{ $labels.pod }} cannot publish audit events"
      description: "Audit events from {{ $labels.pod }} could not be published to the RabbitMQ server. Check the pod log for details. Once the underlying issue was addressed, delete the offending pod to clear this alert."

  - alert: OpenstackLimesIncompleteProjectResourceData
    expr: max by (service, resource) (limes_project_resources_by_type_count) != on () group_left max(limes_project_count)
    # Do not trigger too fast! When a new resource is added, it takes one full scrape cycle to create all resource records.
    for: 60m
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
      context: data-completeness
      meta: "Incomplete project resource data for {{$labels.service}}/{{$labels.resource}}"
      playbook: docs/support/playbook/limes/alerts/incomplete_project_resource_data
    annotations:
      summary: Incomplete project resource data in Limes DB
      description: Some or all projects are missing an entry in the "project_resources" table for resource
        {{$labels.service}}/{{$labels.resource}}. Until this is resolved, PUT requests on the API may fail. Check the
        log for limes-collect; its consistency check should create any missing entries. If this alert only appears for
        baremetal flavor resources, try restarting all nova-api pods, then afterwards restart all limes-api and
        limes-collect pods.

  - alert: OpenstackLimesMismatchProjectQuota
    expr: limes_mismatch_project_quota_count > 0
    for: 60m # every 30m, limes-collect scrapes quota/usage on each project service and at the same time tries to rectify this error; give it 1-2 chances before alerting
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Mismatched Project Quota
      description: Limes detected that the quota of some resource(s) in some project differ from the backend quota for that resource and project.
        This may happen when Limes is unable to write a changed quota value into the backend, for example because of a service downtime.

        More details can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/resources/cluster/current#/inconsistencies|the Limes Inconsistencies view in Elektra>.

  - alert: OpenstackLimesOverspentProjectQuota
    expr: limes_overspent_project_quota_count > 0
    for: 60m # every 30m, limes-collect scrapes quota/usage on each project service; give it 1-2 chances to observe a consistent usage value before alerting
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Overspent Project Quota
      description: Limes detected that the quota of some resource in some project is lower than the usage of that resource in that project.
        This may happen when someone else changes the quota in the backend service directly and increases usage before Limes intervenes, or when a cloud administrator changes quota constraints.

        More details can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/resources/cluster/current#/inconsistencies|the Limes Inconsistencies view in Elektra>.

  - alert: OpenstackNonGrowingQuotaAlmostExhausted
    expr: max by (support_group, service_name, service, resource, domain, project_id, project) (
        ((limes_project_usage{resource!~"instances_.*"} > 0.9 * limes_project_quota) and on (service, resource) (limes_autogrow_growth_multiplier == 1)) * on (service) group_left (support_group) (limes_support_group_mapping)
      )
    for: 5m
    labels:
      severity: warning
      support_group: '{{ $labels.support_group }}'
      tier: os
      service: '{{ $labels.service_name }}'
      playbook: docs/support/playbook/limes/alerts/non_growing_quota
    annotations:
      summary: Non-growing quota almost exhausted
      description: |
        The {{ $labels.service }}/{{ $labels.resource }} quota in project {{ $labels.project_id }} ("{{ $labels.domain }}/{{ $labels.project }}") is more than 90% used.
        This quota does not grow automatically, so care should be taken now to ensure that the customer does not run out of quota.
        Please check the playbook for which options you have to resolve this issue.
