# vim: set ft=yaml:

groups:
- name: openstack-limes.alerts
  rules:

  # This alert is used for detecting unusually high API load before it becomes
  # a problem. We don't care about the collector; it self-regulates by slowing
  # down when its connection pool is exhausted.
  - alert: OpenstackLimesDBConnectionPoolNearlyFull
    expr: avg_over_time(go_sql_stats_connections_in_use{db_name="limes",kubernetes_pod_name!~"limes-collect-.*"}[1h]) > 8
    for: 5m
    labels:
      context: dbconnpool
      dashboard: limes-overview
      service: limes
      severity: info
      tier: os
      meta: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
    annotations:
      summary: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
      description: |
        The DB connection pool on pod {{ $labels.kubernetes_pod_name }} is filling up. It can
        go up to 16 connections, but during regular operations we should not go
        over 3-5 connections to retain some budget for request spikes. Going
        high on connections for a long time indicates that the pod might be
        starved for CPU time, so try checking the CPU throttling metrics.

  - alert: OpenstackLimesHttpErrors
    expr: sum(increase(http_requests_total{kubernetes_namespace="limes",code=~"5.*"}[1h])) by (kubernetes_name) > 0
    for: 5m
    labels:
      context: api
      dashboard: limes-overview
      service: limes
      severity: info
      tier: os
    annotations:
      description: "{{ $labels.kubernetes_name }} is producing HTTP responses with 5xx status codes."
      summary: Server errors on {{ $labels.kubernetes_name }}

  - alert: OpenstackLimesNotScraping
    expr: absent(rate(limes_successful_scrapes{os_cluster="ccloud"}[30m]) > 0)
    for: 5m
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      tier: os
    annotations:
      description: There have been no successful scrapes in the last hour in
        the ccloud cluster, so limes-collect-ccloud is probably dead.
      summary: Limes is not scraping

  - alert: OpenstackLimesFailedCapacityScrapes
    expr: sum(increase(limes_failed_capacity_scrapes[5m])) BY (os_cluster, capacitor) > 0
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      tier: os
    annotations:
      description: Limes cannot scrape capapcity from {{ title $labels.capacitor }}
        for more than an hour.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} contain additional info.
      summary: Limes cannot scrape capacity {{ title $labels.capacitor }}

  - alert: OpenstackLimesMissingCapacity
    # This ignores baremetal capacity ("compute/instances_<flavorname>")
    # since many of these are zero legitimately (we do not have all BM server
    # types in all regions).
    #
    # The check for non-zero domain quota skips resources that only report
    # usage, not quota nor capacity (at the moment, those are
    # "compute/{cores,ram}_{bigvm,regular}").
    expr: global:limes_consolidated_cluster_capacity{full_resource!~"compute/instances_.*"} == 0 and on (full_resource, os_cluster) global:limes_consolidated_domain_quota{full_resource!~"compute/instances_*"} > 0
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: info
      meta: 'no capacity for {{ $labels.full_resource }}'
      tier: os
    annotations:
      description: Limes reports no capacity for {{ $labels.full_resource }}.
        This usually means that the backend service reported weirdly-shaped data
        to Limes' capacity scanner.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} may contain additional info.
      summary: Limes reports zero capacity for {{ $labels.full_resource }}

  - alert: OpenstackLimesFailedScrapes
    expr: sum(increase(limes_failed_scrapes[5m])) BY (os_cluster, service, service_name) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      tier: os
      playbook: docs/support/playbook/limes/failed_scrapes.html
    annotations:
      description: Limes cannot scrape data from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} contain additional info.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesSuspendedScrapes
    # this has [15m] instead of [5m] because suspensions last for 10 minutes,
    # therefore the limes_suspended_scrapes counter only increments that often
    expr: sum(increase(limes_suspended_scrapes[15m])) BY (os_cluster, service, service_name) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      tier: os
    annotations:
      description: Limes has suspended scraping from {{ title $labels.service_name }}
        for more than an hour. Please check if the internal endpoint for
        {{ title $labels.service_name }} is correctly entered in the Keystone catalog.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} contain additional info.
      summary: Limes cannot find {{ title $labels.service_name }} endpoint

  - alert: OpenstackLimesFailedRateScrapes
    expr: sum(increase(limes_failed_rate_scrapes[5m])) BY (os_cluster, service, service_name) > 0
    for: 1h
    labels:
      context: failedratescrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      tier: os
      playbook: docs/support/playbook/limes/failed_scrapes.html
    annotations:
      description: Limes cannot scrape rate data from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} contain additional info.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesSuspendedRateScrapes
    # this has [15m] instead of [5m] because suspensions last for 10 minutes,
    # therefore the limes_suspended_rate_scrapes counter only increments that often
    expr: sum(increase(limes_suspended_rate_scrapes[15m])) BY (os_cluster, service, service_name) > 0
    for: 1h
    labels:
      context: failedratescrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: warning
      tier: os
    annotations:
      description: Limes has suspended rate scraping from {{ title $labels.service_name }}
        for more than an hour. Please check if the internal endpoint for
        {{ title $labels.service_name }} is correctly entered in the Keystone catalog.
        The `kubectl logs` for limes-collect-{{ $labels.os_cluster }} contain additional info.
      summary: Limes cannot find {{ title $labels.service_name }} endpoint

  - alert: OpenstackLimesMissingSwiftMetrics
    expr: (limes_project_usage{service="object-store",resource="capacity"} > 0) unless on(project_id) (limes_plugin_metrics_ok{service="object-store"} == 1)
    for: 5m
    labels:
      context: swiftmetrics
      dashboard: limes-overview
      service: limes
      severity: warning
      tier: os
      meta: "project {{ $labels.project_id }}"
    annotations:
      description: The project {{ $labels.domain }}/{{ $labels.project }} (UUID {{ $labels.project_id }})
        has Swift containers, but Limes does not submit Swift usage metrics to Maia.
        Check the `kubectl logs` for limes-collect-{{ $labels.os_cluster }} for additional info.
      summary: Limes cannot submit Swift usage metrics

  - alert: OpenstackLimesFailedDomainDiscoveries
    expr: sum(increase(limes_failed_domain_discoveries[5m])) BY (os_cluster) > 0
    for: 30m
    labels:
      context: faileddiscoveries
      dashboard: limes-overview
      service: limes
      severity: warning
      tier: os
    annotations:
      description: Limes cannot discover domains for cluster {{ $labels.os_cluster }}.
        Please check if "openstack domain list" is working.
      summary: Limes cannot discover domains.

  - alert: OpenstackLimesFailedProjectDiscoveries
    expr: sum(increase(limes_failed_project_discoveries[5m])) BY (os_cluster, domain) > 0
    for: 30m
    labels:
      context: faileddiscoveries
      dashboard: limes-overview
      service: limes
      severity: warning
      tier: os
    annotations:
      description: Limes cannot discover projects in domain {{ $labels.domain }}
        of cluster {{ $labels.os_cluster }}. Please check if "openstack project
        list --domain {{ $labels.domain }}" is working.
      summary: Limes cannot discover projects.

  - alert: OpenstackLimesAuditEventPublishFailing
    expr: sum(increase(limes_failed_auditevent_publish[10m])) > 0
    for: 1h
    labels:
      context: auditeventpublish
      dashboard: limes-overview
      service: limes
      severity: info
      tier: os
    annotations:
      description: "Audit events could not be published to the RabbitMQ server."
      summary: "Audit events publish failing"

  - alert: OpenstackLimesPodSchedulingFailedInsufficientCPU
    expr: sum(rate(kube_pod_failed_scheduling_cpu_total{pod_name=~"limes-.+"}[30m])) by (pod_name) > 0
    for: 15m
    labels:
      severity: warning
      tier: os
      service: limes
      context: cpu
      dashboard: limes-overview
      meta: "{{ $labels.pod_name }}"
      no_alert_on_absence: "true" # the underlying metric is only generated when scheduling fails
    annotations:
      summary: Scheduling failed due to insufficient cpu
      description: The pod {{ $labels.pod_name }} failed to be scheduled. Insuffient CPU!

  - alert: OpenstackLimesSeesHypervisorWithoutAZ
    # The max_over_time() is used to avoid alert flapping when the collector restarts e.g. after an OOM kill.
    expr: max by (hypervisor, hostname) (max_over_time(limes_nova_hypervisor_has_az{region!~"qa-de-.*"}[10m])) == 0 # only active in prod regions
    for: 15m
    labels:
      severity: info
      tier: os
      service: limes
      context: nova-hypervisor
      meta: "Nova hypervisor {{$labels.hypervisor}} is not assigned to any AZ."
    annotations:
      summary: Unmatched Nova hypervisors
      description: Limes cannot determine the AZ of the hypervisor
        {{$labels.hypervisor}}. Its capacity will be reported in the AZ
        "unknown" in the resource management UI until this issue is fixed.
