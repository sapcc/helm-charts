# vim: set ft=yaml:

groups:
- name: openstack-limes.alerts
  rules:

  # This alert is used for detecting unusually high API load before it becomes
  # a problem. We don't care about the collector; it self-regulates by slowing
  # down when its connection pool is exhausted.
  - alert: OpenstackLimesDBConnectionPoolNearlyFull
    expr: avg_over_time(go_sql_stats_connections_in_use{db_name="limes",kubernetes_pod_name!~"limes-collect-.*"}[1h]) > 8
    for: 5m
    labels:
      context: dbconnpool
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
      meta: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
    annotations:
      summary: 'DB connection pool nearly full on {{ $labels.kubernetes_pod_name }}'
      description: |
        The DB connection pool on pod {{ $labels.kubernetes_pod_name }} is filling up. It can
        go up to 16 connections, but during regular operations we should not go
        over 3-5 connections to retain some budget for request spikes. Going
        high on connections for a long time indicates that the pod might be
        starved for CPU time, so try checking the CPU throttling metrics.

  - alert: OpenstackLimesHttpErrors
    expr: sum(increase(http_requests_total{kubernetes_namespace="limes",code=~"5.*"}[1h])) by (kubernetes_name) > 0
    for: 5m
    labels:
      context: api
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
    annotations:
      description: "{{ $labels.kubernetes_name }} is producing HTTP responses with 5xx status codes."
      summary: Server errors on {{ $labels.kubernetes_name }}

  - alert: OpenstackLimesNotScraping
    expr: absent(rate(limes_scrapes{task_outcome="success"}[30m]) > 0)
    for: 5m
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: There have been no successful scrapes in the last hour,
        so limes-collect-ccloud is probably dead.
      summary: Limes is not scraping

  - alert: OpenstackLimesFailedCapacityScrapes
    expr: sum(increase(limes_capacity_scrapes{task_outcome="failure"}[5m])) BY (service_type) > 0
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: Limes cannot scrape capacity from {{ title $labels.service_type }}
        for more than an hour.
        The `kubectl logs` for limes-collect-ccloud contain additional info.
      summary: Limes cannot scrape capacity {{ title $labels.service_type }}

  - alert: OpenstackLimesMissingCapacity
    # This excludes:
    # - (explicitly) NoQuota resources
    # - (implicitly) resources that technically exist, but legitimately do not have capacity
    #   (e.g. HANA flavors for hypervisor types that do not exist in the region)
    expr: (global:limes_consolidated_cluster_capacity{full_resource!="sharev2/snapmirror_capacity"} == 0) and (max by (full_resource) (global:limes_consolidated_domain_usage) > 0)
    for: 1h
    labels:
      context: failedcapacityscrapes
      dashboard: limes-overview
      service: limes
      severity: info
      meta: 'no capacity for {{ $labels.full_resource }}'
      support_group: containers
      tier: os
    annotations:
      description: Limes reports no capacity for {{ $labels.full_resource }} even though there is assigned domain quota and/or usage.
        This usually happens because the backend service reported weirdly-shaped data to Limes' capacity scanner.
        The `kubectl logs` for limes-collect-ccloud may contain additional info.
        If not, running `limes test-scan-capacity` on the respective region and service_type may provide additional insight.
      summary: Limes reports zero capacity for {{ $labels.full_resource }}

  - alert: OpenstackLimesFailedScrapes
    expr: sum(increase(limes_scrapes{task_outcome="failure"}[5m])) BY (service_type, service_name, region) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: '{{ $labels.service_name }}'
      severity: info
      support_group: '{{ if eq $labels.service_name "cinder" "ironic" "nova" "manila" -}} compute-storage-api {{- else if eq $labels.service_name "archer" "designate" "neutron" "octavia" -}} network-api {{- else if eq $labels.service_name "ceph" "swift" -}} storage {{- else -}} containers {{- end -}}'
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape data from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesFailingScrapes
    expr: max(limes_failing_scrapes_gauge) BY (service_name, region) > 0
    for: 1h
    labels:
      context: failedscrapes
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
      playbook: docs/support/playbook/limes/alerts/failed_scrapes
    annotations:
      description: Limes cannot scrape data for projects from {{ title $labels.service_name }}
        for more than an hour. Please check if {{ title $labels.service_name }} is working.
        Error messages can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/cc-tools/limes#/scrape-errors|the Limes Errors view in Elektra>.
      summary: Limes cannot scrape {{ title $labels.service_name }}

  - alert: OpenstackLimesMissingSwiftMetrics
    expr: (limes_project_usage{service="object-store",resource="capacity"} > 0) unless on(project_id) (limes_usage_collection_metrics_ok{service="swift"} == 1)
    for: 5m
    labels:
      context: swiftmetrics
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
      meta: "project {{ $labels.project_id }}"
    annotations:
      description: The project {{ $labels.domain }}/{{ $labels.project }} (UUID {{ $labels.project_id }})
        has Swift containers, but Limes does not submit Swift usage metrics to Maia.
        Check the `kubectl logs` for limes-collect-ccloud for additional info.
      summary: Limes cannot submit Swift usage metrics

  - alert: OpenstackLimesFailedKeystoneSyncs
    expr: sum(increase(limes_keystone_syncs{task_outcome="failure"}[5m])) > 0 or sum(increase(limes_keystone_syncs{task_outcome="success"}[5m])) == 0
    for: 30m
    labels:
      context: keystonesync
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
      tier: os
    annotations:
      description: Limes cannot sync domains and projects from Keystone.
        Check the `kubectl logs` for limes-collect-ccloud for additional info.
      summary: Limes cannot sync from Keystone.

  - alert: OpenstackLimesFailedQuotaOverrideSyncs
    expr: sum(increase(limes_quota_override_syncs{task_outcome="failure"}[10m])) > 0 or sum(increase(limes_quota_override_syncs{task_outcome="success"}[10m])) == 0
    for: 10m
    labels:
      context: quotaoverridesync
      dashboard: limes-overview
      service: limes
      severity: warning
      support_group: containers
    annotations:
      description: Limes cannot sync quota overrides from its config into the DB.
        Check the `kubectl logs` for limes-collect-ccloud for errors (esp. from parsing the overrides file).
      summary: Limes cannot sync quota overrides.

  - alert: OpenstackLimesAuditEventPublishFailing
    # The underlying metric counts failed submission attempts, e.g. because the hermes-rabbitmq server is restarting.
    # These are not necessarily fatal because the process will hold them in memory to retry the submission later.
    # The alert will clear up on its own once submissions start working again.
    expr: sum by (pod) (changes(audittools_failed_submissions{namespace="limes"}[1h]) > 0)
    for: 5m
    labels:
      context: auditeventpublish
      dashboard: limes-overview
      service: limes
      severity: info
      support_group: containers
      tier: os
      meta: '{{ $labels.pod }}'
    annotations:
      summary: "{{ $labels.pod }} cannot publish audit events"
      description: "Audit events from {{ $labels.pod }} could not be published to the RabbitMQ server. Check the pod log for detailed error messages. Affected audit events are held in memory until publishing succeeds."

  - alert: OpenstackLimesIncompleteProjectResourceData
    expr: max by (service, resource, namespace) (limes_project_resources_by_type_count) != on (namespace) group_left max by (namespace) (limes_project_count)
    # Do not trigger too fast! When a new resource is added, it takes one full scrape cycle to create all resource records.
    for: 60m
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
      context: data-completeness
      meta: "Incomplete project resource data for {{$labels.service}}/{{$labels.resource}}"
      playbook: docs/support/playbook/limes/alerts/incomplete_project_resource_data
    annotations:
      summary: Incomplete project resource data in Limes DB
      description: Some or all projects are missing an entry in the "project_resources" table for resource
        {{$labels.service}}/{{$labels.resource}}. Until this is resolved, PUT requests on the API may fail. Check the
        log for limes-collect; its consistency check should create any missing entries. If this alert only appears for
        baremetal flavor resources, try restarting all nova-api pods, then afterwards restart all limes-api and
        limes-collect pods.

  - alert: OpenstackLimesMismatchProjectQuota
    expr: max by (service_name) (limes_mismatch_project_quota_count > 0)
    for: 60m # every 30m, limes-collect scrapes quota/usage on each project service and at the same time tries to rectify this error; give it 1-2 chances before alerting
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Mismatched Project Quota
      description: Limes detected that the quota of some {{ $labels.service_name }} resource(s) in some project differ from the backend quota for that resource and project.
        This may happen when Limes is unable to write a changed quota value into the backend, for example because of a service downtime.

        More details can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/resources/cluster/current#/inconsistencies|the Limes Inconsistencies view in Elektra>.

  - alert: OpenstackLimesOverspentProjectQuota
    expr: max(limes_overspent_project_quota_count > 0)
    for: 60m # every 30m, limes-collect scrapes quota/usage on each project service; give it 1-2 chances to observe a consistent usage value before alerting
    labels:
      severity: info
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Overspent Project Quota
      description: Limes detected that the quota of some resource in some project is lower than the usage of that resource in that project.
        This may happen when someone else changes the quota in the backend service directly and increases usage before Limes intervenes, or when a cloud administrator changes quota constraints.

        More details can be found in <https://dashboard.{{ $labels.region }}.cloud.sap/ccadmin/cloud_admin/resources/cluster/current#/inconsistencies|the Limes Inconsistencies view in Elektra>.

  - alert: OpenstackNonGrowingQuotaAlmostExhausted
    expr: max by (support_group, service_name, service, resource, domain, project_id, project) (
        ((limes_project_usage{resource!~"instances_.*"} > 0.9 * limes_project_quota) and on (service, resource) (limes_autogrow_growth_multiplier == 1)) * on (service) group_left (support_group) (limes_support_group_mapping)
      ) unless on (service, resource) (limes_available_commitment_duration == 1)
    for: 5m
    labels:
      severity: warning
      support_group: '{{ $labels.support_group }}'
      tier: os
      service: '{{ $labels.service_name }}'
      playbook: docs/support/playbook/limes/alerts/non_growing_quota
    annotations:
      summary: Non-growing quota almost exhausted
      description: |
        The {{ $labels.service }}/{{ $labels.resource }} quota in project {{ $labels.project_id }} ("{{ $labels.domain }}/{{ $labels.project }}") is more than 90% used.
        This quota does not grow automatically, so care should be taken now to ensure that the customer does not run out of quota.
        Please check the playbook for which options you have to resolve this issue.

  - alert: OpenstackLimesIneffectiveQuotaOverride
    expr: 'max by (domain, project, project_id, service, resource) (limes_project_quota != limes_project_override_quota_from_config)'
    for: 60m
    labels:
      severity: warning
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Quota override is ineffective
      description: |
        In the project {{ $labels.project_id }} ({{ $labels.domain }}/{{ $labels.project }}), the quota override for {{ $labels.service }}/{{ $labels.resource }} is ineffective.
        The actual quota is {{ $value }}, which differs from the configured quota override.
        Please check the Resource Management UI of the project. If the quota override cannot be enforced because commitments or usage are higher than it, the override needs to be removed (from quota-overrides.yaml in cc/secrets).
        The opposite case (a quota below the configured override) needs to be triaged by a Limes expert.

  - alert: OpenstackLimesCinderCapacityWithUnknownVolumeType
    # NOTE: This alert is disabled in QA because Tempest runs create private volume types all the time.
    expr: 'sum by (project_id) ((liquid_cinder_snapshots_with_unknown_volume_type_size{region!~"qa.*"} > 0) + (liquid_cinder_volumes_with_unknown_volume_type_size{region!~"qa.*"} > 0))'
    for: 15m
    labels:
      severity: warning
      support_group: containers
      tier: os
      service: limes
    annotations:
      summary: Cinder capacity with unknown volume type
      description: |
        The project {{ $labels.project_id }} contains Cinder volumes and/or snapshots whose volume types are not known to Limes (which usually means that they are private).
        This might be bad because payload without a known volume type is not billed.
        Please check the log of the liquid-cinder pod in the limes namespace to find out which volumes/snapshots are problematic and which volume types they are using.
