global:
  linkerd_requested: true
  is_global_region: false
  domain_seeds:
    customer_domains: []
    customer_domains_without_support_projects: []

campfire:
  mail_type:
    expire:
      subject: "Information about expiring commitments"
      condition: "about to expire"
    confirmed:
      subject: "Information about confirmed commitments"
      condition: "confirmed and ready to use"

linkerd-support:
  annotate_namespace: true

owner-info:
  support-group: containers
  service: limes
  maintainers:
    - Stefan Majewsky
    - Stefan Voigt
    - Sandro JÃ¤ckel
  helm-chart-url: https://github.com/sapcc/helm-charts/tree/master/openstack/limes

limes:
  image_tag: DEFINED_IN_VALUES_FILE
  has_audit_trail: true

  # List of liquids that we will deploy as part of this Helm release by running `limes liquid $NAME`.
  # The "skip" field can be set to true in regions that do not have the service in question
  local_liquids:
    archer:    { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    cinder:    { skip: false, cpu_request: 100m, memory_limit: 128Mi }
    cronus:    { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    designate: { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    ironic:    { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    manila:    { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    nova:      { skip: false, cpu_request: 100m, memory_limit: 256Mi } # initial high estimate, TODO: adjust downwards if prod deployments end up having unused headroom
    neutron:   { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    octavia:   { skip: false, cpu_request:  20m, memory_limit:  64Mi }
    swift:     { skip: false, cpu_request:  20m, memory_limit:  64Mi }

  # Out of the local liquids above, some take configuration, which is passed in these sections.
  local_liquid_configs:
    cinder:
      with_subcapacities: true
      with_volume_subresources: true
      with_snapshot_subresources: true
      ignore_public_volume_types: "tempest.+"
      manage_private_volume_types: "kvm-pilot"

    ironic:
      with_subcapacities: true
      with_subresources: true

    manila:
      capacity_calculation:
        capacity_balance: 0.75
        share_networks: 250
        shares_per_pool: 1000
        snapshots_per_share: 5
        with_subcapacities: true
      prometheus_api_for_az_awareness:
        url: http://prometheus-openstack.prometheus-openstack.svc:9090
      prometheus_api_for_netapp_metrics:
        url: http://prometheus-storage.infra-monitoring.svc:9090
      share_types: [] # see regional values.yaml

    nova:
      with_subcapacities: true
      with_subresources: true
      flavor_selection:
        required_extra_specs: { "vmware:hv_enabled" : "True" } # consider only VMware flavors
      hypervisor_selection:
        hypervisor_type_pattern: '(?i)vmware'                  # consider only VMware hypervisors
        shadowing_traits: [ CUSTOM_DECOMMISSIONING ]
        aggregate_name_pattern: '^vc-[a-z]-[0-9]+$'            # AZ association is established by the vCenter-membership aggregates
      binpack_behavior:
        # We are currently running with a sort of lobotomized placement scoring function: In short, placeability of HANA flavors is dominated by the available VCPUs.
        # Memory follows the same curve, but there is a bit of leftovers on full nodes, which makes the scoring prefer empty nodes over half-full nodes.
        # This is even worse on the disk dimension, because there is a gigantic amount of leftovers for local disk even on full nodes.
        score_ignores_disk: true
        score_ignores_ram: true
      ignored_traits: [ CUSTOM_HW_SAPPHIRE_RAPIDS ]

  # Map with entries like:
  #
  #   cluster_id:
  #     auth_password: swordfish
  #     rabbitmq_user: hare
  #     rabbitmq_password: blowfish
  passwords: DEFINED_IN_VALUES_FILE

  # This section of the YAML must be identical to the "clusters" section of the
  # Limes configuration file.
  # <https://github.com/sapcc/limes/blob/master/docs/operators/config.md>
  clusters:
    ccloud: {}

  # Additional role assignments for the respective roles to external users of the Limes API.
  # Each entry must be an object with the keys "user_name", "user_domain_name", "project_name", "project_domain_name".
  external_role_assignments:
    cloud_resource_admin: []
    cloud_resource_viewer: []

  # Whether to apply resource requests/limits to containers.
  resources:
    enabled: false

postgresql:
  persistence:
    enabled: true
    size: 10Gi
  databases:
    limes: {}
  users:
    limes: {}
  sharedMemoryLimit: 128Mi

  config:
    log_min_duration_statement: 250
    # less than the postgresql chart's default; I want to know early when connections start getting out of hand
    max_connections: 64

  alerts:
    support_group: containers

pgbackup:
  alerts:
    support_group: containers

pgmetrics:
  alerts:
    support_group: containers

  collectors:
    stat_bgwriter: false

  databases:
    limes:
      customMetrics:
        limes_failing_scrapes:
          # The `SUM(...)` has mostly the same effect as `COUNT(*) ... WHERE
          # ps.scrape_error_message != ''`, but generates zero-valued results for
          # all service types without scrape errors. This ensures that an absence
          # of errors does not trigger a metric absence alert.
          query: >
            SELECT s.type AS service_name, SUM(CASE WHEN ps.scrape_error_message = '' THEN 0 ELSE 1 END) AS gauge
            FROM project_services ps
            JOIN services s ON s.id = ps.service_id
            GROUP BY s.type
          metrics:
            - service_name:
                usage: "LABEL"
                description: "Service type"
            - gauge:
                usage: "GAUGE"
                description: "Total number of projects that are failing quota/usage scrape"

        limes_mismatch_project_quota:
          # The UNION SELECT adds a dummy row to satisfy the absent-metrics-operator.
          query: >
            SELECT s.type AS service_name, COUNT(*) as count
            FROM project_resources pr
            JOIN resources r ON r.id = pr.resource_id
            JOIN services s ON s.id = r.service_id
            WHERE pr.backend_quota != pr.quota
            GROUP BY s.type
            UNION SELECT 'none' AS service_name, 0 AS count
          metrics:
            - service_name:
                usage: "LABEL"
                description: "Service type"
            - count:
                usage: "GAUGE"
                description: "Total number of project resources that have mismatched quota"

        limes_overspent_project_quota:
          # Swift does quota checks decentralized, in the individual proxy instances. When multiple parallel uploads each
          # fit into the remaining quota, they will therefore all be permitted, even if the total sum exceeds the available
          # quota. This causes overspent quotas on a fairly regular basis, and we don't really want (or need) to do anything
          # about it. Just like eventual consistency in general, this is just how Swift behaves. Therefore this metric only
          # considers quota inconsistencies in Swift if they are large (in specific, larger than 1 TiB = 1099511627776 byte).
          query: >
            WITH tmp AS (
              SELECT 1
              FROM project_az_resources pazr
              JOIN az_resources azr ON azr.id = pazr.az_resource_id
              JOIN resources r ON r.id = azr.resource_id
              JOIN project_resources pr ON pr.resource_id = r.id AND pr.project_id = pazr.project_id
              GROUP BY pr.project_id, r.path, pr.quota
              HAVING SUM(pazr.usage) > pr.quota
                AND NOT (r.path = 'swift/capacity' AND SUM(pazr.usage) < pr.quota + 1099511627776)
            )
            SELECT COUNT(*) AS count FROM tmp
          metrics:
            - count:
                usage: "GAUGE"
                description: "Total number of project resources that have overspent quota"

        limes_project:
          query: >
            SELECT COUNT(*) AS count FROM projects
          metrics:
            - count:
                usage: "GAUGE"
                description: "Number of projects in the Limes DB"

        limes_project_resources_by_type:
          query: >
            SELECT s.type AS service, r.name AS resource, COUNT(*) AS count
            FROM project_resources pr
            JOIN resources r ON r.id = pr.resource_id
            JOIN services s ON s.id = r.service_id
            GROUP BY s.type, r.name
          metrics:
            - service:
                usage: "LABEL"
                description: "Service type"
            - resource:
                usage: "LABEL"
                description: "Resource name"
            - count:
                usage: "GAUGE"
                description: "Number of projects that have a project_resources entry for this service and resource"

        # This metric is federated into prometheus-kubernetes for alerting in
        # Kubernikus. (Low free Swift quota means Kubernikus might become unable to
        # do etcd backups.)
        limes_free_swift_quota:
          query: >
            SELECT p.uuid AS project_id, p.name AS project, d.name AS domain, GREATEST(pr.backend_quota - pazr.usage, 0) AS gauge
            FROM domains d
            JOIN projects p ON p.domain_id = d.id
            JOIN project_az_resources pazr ON pazr.project_id = p.id
            JOIN az_resources azr ON azr.id = pazr.az_resource_id
            JOIN resources r ON r.id = azr.resource_id
            JOIN project_resources pr ON pr.resource_id = r.id AND pr.project_id = pazr.project_id
            WHERE r.path = 'swift/capacity' AND azr.az = 'any'
          metrics:
            - gauge:
                usage: "GAUGE"
                description: "Free backend quota for Swift capacity"
            - project_id:
                usage: "LABEL"
                description: "Project ID"
            - project:
                usage: "LABEL"
                description: "Project name"
            - domain:
                usage: "LABEL"
                description: "Domain name"

        limes_support_group:
          query: >
            SELECT 1 AS mapping, type AS service, CASE
              WHEN type IN ('compute', 'cinder', 'ironic', 'manila', 'nova') THEN 'compute-storage-api'
              WHEN type IN ('archer', 'designate', 'neutron', 'octavia')     THEN 'network-api'
              WHEN type IN ('cronus')                                        THEN 'email'
              WHEN type IN ('ceph', 'swift')                                 THEN 'storage'
              ELSE 'containers' END AS support_group
            FROM services GROUP BY type
          metrics:
            - mapping:
                usage: "GAUGE"
                description: "Mapping between Limes service types and support group names"
            - service:
                usage: "LABEL"
                description: "Service type in Limes"
            - support_group:
                usage: "LABEL"
                description: "Support group for alerts"

# Deploy limes Prometheus alerts.
alerts:
  enabled: true
  # Names of the Prometheus to which the alerts should be assigned to.
  # Keys = directory names in alerts/ and aggregations/
  prometheus:
    openstack: openstack
    kubernetes: kubernetes
