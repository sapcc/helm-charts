global:
  region: DEFINED_IN_VALUES_FILE
  linkerd_requested: true

  domain_seeds:
    skip_hcm_domain: false

linkerd-support:
  annotate_namespace: true

owner-info:
  support-group: containers
  service: limes
  maintainers:
    - Stefan Majewsky
    - Stefan Voigt
    - Sandro JÃ¤ckel
  helm-chart-url: https://github.com/sapcc/helm-charts/tree/master/openstack/limes

limes:
  image_tag: DEFINED_IN_VALUES_FILE
  has_audit_trail: true

  # Map with entries like:
  #
  #   cluster_id:
  #     auth_password: swordfish
  #     rabbitmq_user: hare
  #     rabbitmq_password: blowfish
  passwords: DEFINED_IN_VALUES_FILE

  # This section of the YAML must be identical to the "clusters" section of the
  # Limes configuration file.
  # <https://github.com/sapcc/limes/blob/master/docs/operators/config.md>
  clusters:
    ccloud: {}

  # Map with entries being the contents of a Limes quota constraint file.
  # <https://github.com/sapcc/limes/blob/master/docs/operators/constraints.md>
  # e.g.
  #
  #   constraints:
  #     ccloud:
  #       domains: ...
  #       projects: ...
  #
  # To use this constraint in a cluster, set:
  #
  #    .Values.limes.clusters.ccloud.constraints = "/etc/limes/constraints-ccloud.yaml"
  constraints:
    ccloud: {}

  # Whether to apply resource requests/limits to containers.
  resources:
    enabled: false

postgresql:
  persistence:
    enabled: true
    size: 10Gi
  postgresDatabase: limes
  users:
    limes: {}
  tableOwner: limes

  config:
    log_min_duration_statement: 250
    # less than the postgresql chart's default; I want to know early when connections start getting out of hand
    max_connections: 64

  alerts:
    support_group: containers

pgbackup:
  isPostgresNG: true
  database:
    name: limes
  alerts:
    support_group: containers

pgmetrics:
  isPostgresNG: true
  db_name: limes

  alerts:
    support_group: containers

  collectors:
    stat_bgwriter: false

  customMetrics:
    limes_failing_scrapes:
      # The `SUM(...)` has mostly the same effect as `COUNT(*) ... WHERE
      # ps.scrape_error_message != ''`, but generates zero-valued results for
      # all service types without scrape errors. This ensures that an absence
      # of errors does not trigger a metric absence alert.
      query: >
        SELECT ps.type AS service_name, SUM(CASE WHEN ps.scrape_error_message = '' THEN 0 ELSE 1 END) AS gauge
          FROM project_services ps
         GROUP BY ps.type
      metrics:
        - service_name:
            usage: "LABEL"
            description: "Service type"
        - gauge:
            usage: "GAUGE"
            description: "Total number of projects that are failing quota/usage scrape"

    limes_failing_rate_scrapes:
      # Same strategy as above for `limes_failing_scrapes`.
      query: >
        SELECT ps.type AS service_name, SUM(CASE WHEN ps.rates_scrape_error_message = '' THEN 0 ELSE 1 END) AS gauge
          FROM project_services ps
         GROUP BY ps.type
      metrics:
        - service_name:
            usage: "LABEL"
            description: "Service type"
        - gauge:
            usage: "GAUGE"
            description: "Total number of projects that are failing rate data scrape"

    limes_mismatch_project_quota:
      query: >
        SELECT COUNT(*) as count FROM project_resources
        WHERE backend_quota != quota
      metrics:
        - count:
            usage: "GAUGE"
            description: "Total number of project resources that have mismatched quota"

    limes_overspent_project_quota:
      # Swift does quota checks decentralized, in the individual proxy instances. When multiple parallel uploads each
      # fit into the remaining quota, they will therefore all be permitted, even if the total sum exceeds the available
      # quota. This causes overspent quotas on a fairly regular basis, and we don't really want (or need) to do anything
      # about it. Just like eventual consistency in general, this is just how Swift behaves. Therefore this metric only
      # considers quota inconsistencies in Swift if they are large (in specific, larger than 1 TiB = 1099511627776 byte).
      query: >
        WITH tmp AS (
          SELECT 1 FROM project_services ps
            JOIN project_resources pr ON pr.service_id = ps.id
            JOIN project_az_resources par ON par.resource_id = pr.id
           GROUP BY ps.id, ps.type, pr.name, pr.quota
          HAVING SUM(par.usage) > pr.quota
             AND NOT (ps.type = 'object-store' AND pr.name = 'capacity' AND SUM(par.usage) < pr.quota + 1099511627776)
        ) SELECT COUNT(*) AS count FROM tmp
      metrics:
        - count:
            usage: "GAUGE"
            description: "Total number of project resources that have overspent quota"

    limes_project:
      query: >
        SELECT COUNT(*) AS count FROM projects
      metrics:
        - count:
            usage: "GAUGE"
            description: "Number of projects in the Limes DB"

    limes_project_resources_by_type:
      query: >
        SELECT ps.type AS service, pr.name AS resource, COUNT(*) AS count
          FROM project_resources pr JOIN project_services ps ON ps.id = pr.service_id
         GROUP BY ps.type, pr.name
      metrics:
        - service:
            usage: "LABEL"
            description: "Service type"
        - resource:
            usage: "LABEL"
            description: "Resource name"
        - count:
            usage: "GAUGE"
            description: "Number of projects that have a project_resources entry for this service and resource"

    # This metric is federated into prometheus-kubernetes for alerting in
    # Kubernikus. (Low free Swift quota means Kubernikus might become unable to
    # do etcd backups.)
    limes_free_swift_quota:
      query: >
        SELECT p.uuid AS project_id, p.name AS project, d.name AS domain, GREATEST(pr.backend_quota - par.usage, 0) AS gauge
          FROM domains d
          JOIN projects p ON p.domain_id = d.id
          JOIN project_services ps ON ps.project_id = p.id
          JOIN project_resources pr ON pr.service_id = ps.id
          JOIN project_az_resources par ON par.resource_id = pr.id
         WHERE ps.type = 'object-store' AND par.az = 'any'
      metrics:
        - gauge:
            usage: "GAUGE"
            description: "Free backend quota for Swift capacity"
        - project_id:
            usage: "LABEL"
            description: "Project ID"
        - project:
            usage: "LABEL"
            description: "Project name"
        - domain:
            usage: "LABEL"
            description: "Domain name"

    limes_support_group:
      query: >
        SELECT 1 AS mapping, type AS service, CASE
          WHEN type IN ('compute', 'sharev2', 'volumev2')      THEN 'compute-storage-api'
          WHEN type IN ('network', 'dns', 'endpoint-services') THEN 'network-api'
          WHEN type IN ('email-aws')                           THEN 'email'
          WHEN type IN ('object-store')                        THEN 'storage'
          ELSE 'containers' END AS support_group
        FROM project_services GROUP BY type
      metrics:
        - mapping:
            usage: "GAUGE"
            description: "Mapping between Limes service types and support group names"
        - service:
            usage: "LABEL"
            description: "Service type in Limes"
        - support_group:
            usage: "LABEL"
            description: "Support group for alerts"

# Deploy limes Prometheus alerts.
alerts:
  enabled: true
  # Names of the Prometheus to which the alerts should be assigned to.
  # Keys = directory names in alerts/ and aggregations/
  prometheus:
    openstack: openstack
    kubernetes: kubernetes
