# Default values for percona-xtradb-cluster.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
linkerd:
  enabled: true

owner_info:
  enabled: false

name: null

# NOTE: don't use internal TLS by default, linkerd mesh is preferred
tls:
  enabled: false

unsafeFlags: {}
#unsafeFlags:
#  pxcSize: false
#  proxySize: false
#  backupIfUnhealthy: false

# Enable init.sql job
initdb_job: null
# Set charachter set and collation in init.sql
character_set_server: "utf8mb4"
collation_server: "utf8mb4_0900_ai_ci"

databases: {}
users: {}
#  backup:
#    name: backup
#    password: null
#    limits:
#      max_user_connections: 4
#    grants:
#      - ALL PRIVILEGES ON *.*
#  example:
#    name: example1 # This looks repetitive, but the point is that they key is the name
#                   # you refer to in your charts, while the field 'name' is the actual name
#                   # used as credentials. It should be possible to change the latter,
#                   # without having to change the first.
#    password: null # Causes users not be be created, and even maybe to get locked
#    auth_plugin: 'mysql_native_password'
#    grants:
#    - ALL ON example.*

system_users:
  root:
    password: null
  xtrabackup:
    password: null
  monitor:
    password: null
  proxyadmin:
    password: null
  operator:
    password: null
  replication:
    password: null

ccroot_user:
  enabled: false

ignoreAnnotations: []
ignoreLabels:
  - ccloud/service
  - ccloud/support-group

annotations: {}

pause: false

initContainer:
  image:
    name: percona/percona-xtradb-cluster-operator
    tag: 1.15.0
    override: null
  resources:
    requests:
      memory: 100M
      cpu: 100m
    limits:
      memory: 200M
      cpu: 200m

pxc:
  size: 3
  image:
    name: percona/percona-xtradb-cluster
    tag: 8.0.36-28.1
    override: null
  imagePullPolicy: IfNotPresent
  annotations: {}
  labels:
    system: openstack
    type: database
  readinessProbes:
    initialDelaySeconds: 15
    timeoutSeconds: 15
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 5
  livenessProbes:
    initialDelaySeconds: 300
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  containerSecurityContext:
    privileged: false
  podSecurityContext:
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    supplementalGroups: [1001]
    fsGroupChangePolicy: "Always"
  expose:
    enabled: false
    type: ClusterIP
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    annotations:
      config.linkerd.io/opaque-ports: "3306,3307,3009,4444,4567,4568,33060,33062"
  autoRecovery: true
  configuration:
    performance_schema: true
    options:
      skip-name-resolve: 1
      wsrep_provider_options: "gcache.size=1G; gcache.recover=yes"
      wsrep_applier_threads: 4
      wsrep_retry_autocommit: 3
      binlog_format: ROW
      binlog_expire_logs_seconds: 345600  # default 30 days -> 4 days
      net_read_timeout: 120
      net_write_timeout: 300
      connect_timeout: 30
      wait_timeout: 3800
      interactive_timeout: 1800
      innodb_lock_wait_timeout: 30  # default 50 seconds -> 30 seconds
      max_connections: 1024
      max_connect_errors: "4294967295"  # to avoid failed connections because of loadbalancer health checks
      innodb_flush_log_at_trx_commit: 0
      innodb_flush_method: O_DIRECT
      innodb_file_per_table: 1
      innodb_autoinc_lock_mode: 2
      innodb_buffer_pool_size: 1024M
      innodb_log_file_size: 512M
      innodb_open_files: 4000
      innodb_stats_on_metadata: 0
      innodb_thread_concurrency: 0
      innodb_read_io_threads: 8
      innodb_write_io_threads: 8
      innodb_print_all_deadlocks: 1
      log_bin_trust_function_creators: 1
      slow_query_log: 1
      long_query_time: 5
      log_error_suppression_list: "MY-010055,MY-013360"
  priorityClassName: openstack-service-critical
  affinity:
    advanced:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: cloud.sap/maintenance-state
              operator: In
              values:
              - operational
          weight: 1
        - preference:
            matchExpressions:
            - key: cloud.sap/deployment-state
              operator: NotIn
              values:
              - reinstalling
          weight: 1
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - "{{ .Values.name }}-db"
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - pxc
              topologyKey: topology.kubernetes.io/zone
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - "{{ .Values.name }}-db"
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - pxc
              topologyKey: kubernetes.io/hostname
  resources:
    requests:
      memory: 1G
      cpu: 500m
    limits: {}
  persistence:
    enabled: true
    ## percona data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 10Gi

haproxy:
  enabled: true
  size: 2
  image:
    name: percona/haproxy
    tag: 2.8.5
    override: null
  imagePullPolicy: Always
  annotations: {}
  labels:
    system: openstack
    type: database-proxy
  readinessProbes:
    initialDelaySeconds: 15
    timeoutSeconds: 1
    periodSeconds: 5
    successThreshold: 1
    failureThreshold: 3
  livenessProbes:
    initialDelaySeconds: 60
    timeoutSeconds: 5
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 4
  containerSecurityContext:
    privileged: false
  podSecurityContext:
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    supplementalGroups: [1001]
  service:
    primary:
      enabled: true
      type: ClusterIP
      externalTrafficPolicy: Cluster
      internalTrafficPolicy: Cluster
      annotations:
        config.linkerd.io/opaque-ports: "3306,3307,3009,4444,4567,4568,33060,33062"
    replicas:
      enabled: true
      type: ClusterIP
      externalTrafficPolicy: Cluster
      internalTrafficPolicy: Cluster
      annotations:
        config.linkerd.io/opaque-ports: "3306,3307,3009,4444,4567,4568,33060,33062"
  priorityClassName: openstack-service-critical
  affinity:
    advanced:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: cloud.sap/maintenance-state
              operator: In
              values:
              - operational
          weight: 1
        - preference:
            matchExpressions:
            - key: cloud.sap/deployment-state
              operator: NotIn
              values:
              - reinstalling
          weight: 1
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - "{{ .Values.name }}-db"
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - haproxy
              topologyKey: topology.kubernetes.io/zone
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - "{{ .Values.name }}-db"
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - haproxy
              topologyKey: kubernetes.io/hostname
  resources:
    requests:
      memory: 100M
      cpu: 500m
    limits: {}

initdb:
  image:
    name: percona/percona-xtradb-cluster-operator
    tag: 1.15.0-pxc8.0-backup-pxb8.0.35

backup:
  enabled: false
  annotations: {}
  image:
    name: percona/percona-xtradb-cluster-operator
    tag: 1.15.0-pxc8.0-backup-pxb8.0.35
    override: null
  imagePullPolicy: Always
  backoffLimit: 1
  priorityClassName: openstack-service-critical
  resources:
    requests: {}
    limits: {}
  pitr:
    enabled: false
    annotations: {}
    storageName: s3-backups-binlogs
    timeBetweenUploads: 300
    timeoutSeconds: 60
    priorityClassName: openstack-service-critical
    resources:
      requests: {}
      limits: {}
  s3:
    config:
      credentialsSecret: "pxc-db-{{ .Values.name }}-backup-s3"
      region: "{{ .Values.global.region }}"
      endpointUrl: null
    secrets:
      aws_access_key_id: null
      aws_secret_access_key: null
    storages:
      binlogs:
        enabled: true
        bucket: "pxc-backup-{{ .Values.global.region }}/binlogs/{{.Values.name}}/"
      daily:
        enabled: true
        bucket: "pxc-backup-{{ .Values.global.region }}/backups/{{ .Values.name }}/daily"
      hourly:
        enabled: false
        bucket: "pxc-backup-{{ .Values.global.region }}/backups/{{ .Values.name }}/hourly"
      custom:
        enabled: false
        bucket: "pxc-backup-{{ .Values.global.region }}/backups/{{ .Values.name }}/custom"
  schedule:
    - name: "daily-backup"
      schedule: "0 0 * * *"
      keep: 5
      storageName: s3-backups

# Prometheus metrics.
metrics:
  enabled: true
  flags:
    - collect.binlog_size
    - collect.info_schema.processlist
    - collect.info_schema.query_response_time
  image:
    name: prom/mysqld-exporter
    tag: v0.15.1
    override: null

# Default Prometheus alerts and rules.
alerts:
  enabled: true

  # Name of the Prometheus supposed to scrape the metrics and to which alerts are assigned.
  prometheus: openstack

  # The tier of the alert.
  tier: os

  # The support group label of the alerts. Must be given in values.yaml of parent chart.
  # support_group:

  # Configurable service label of the alerts. Defaults to `.Release.Name`.
  # service:
