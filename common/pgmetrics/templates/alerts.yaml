{{- if .Values.alerts.enabled }}
{{- $service := .Values.alerts.service | default .Release.Name }}

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule

metadata:
  name: {{ .Release.Name }}-pgmetrics-alerts
  labels:
    prometheus: {{ required ".Values.alerts.prometheus missing" .Values.alerts.prometheus }}

spec:
  groups:
  - name: pg-database.alerts
    rules:
    - alert: {{ title $service }}PredictHighNumberOfDatabaseConnections
      expr: predict_linear(pg_stat_activity_count{datname="{{default .Release.Name .Values.db_name}}"}[1h], 3*3600) >= 2000
      for: 5m
      labels:
        context: database
        support_group: {{ required ".Values.alerts.support_group missing" .Values.alerts.support_group }}
        tier: {{ required ".Values.alerts.tier missing" .Values.alerts.tier }}
        service: {{ $service }}
        severity: warning
        meta: "Predicting a high number of database connections for {{`{{ $labels.datname }}`}}"
      annotations:
        summary: High number of database connections
        description: Predicting more than 2000 database connections for {{ $service }} within the next 3 hours and might be unavailable soon.

    - alert: {{ title $service }}StuckTransactions
      expr: max(pg_stat_activity_max_tx_duration{datname="{{default .Release.Name .Values.db_name}}"}) > 600
      for: 5m
      labels:
        context: database
        support_group: {{ required ".Values.alerts.support_group missing" .Values.alerts.support_group }}
        tier: {{ required ".Values.alerts.tier missing" .Values.alerts.tier }}
        service: {{ $service }}
        severity: info
        meta: DB transactions are stuck
      annotations:
        summary: DB transactions are stuck
        description: >
          Some transactions are taking unusually long to complete or not making progress at all.
          Check the logs of the Postgres container; this could be caused by file locks getting stuck.
          To see which transactions are stuck, query for
          `SELECT * FROM pg_stat_activity WHERE state = 'idle in transaction' AND backend_start < NOW() - interval '1 minute'`.

    - alert: {{ title $service }}PostgresMetricsEvalErrors
      expr: max(pg_exporter_last_scrape_error{kubernetes_namespace="{{ .Release.Namespace }}",app="{{ .Release.Name }}-pgmetrics"}) > 0
      for: 5m
      labels:
        context: database
        support_group: {{ required ".Values.alerts.support_group missing" .Values.alerts.support_group }}
        tier: {{ required ".Values.alerts.tier missing" .Values.alerts.tier }}
        service: {{ $service }}
        severity: info
        meta: Evaluation error in Postgres DB metrics
      annotations:
        summary: Evaluation error in Postgres DB metrics
        description: >
          Some of the custom metrics in the postgres-exporter configuration cannot be evaluated. Check the pgmetrics pod log for errors.

    - alert: {{ title $service }}PostgresMetricsSlowCollection
      # slow-moving to avoid flapping during maintenance events etc.
      expr: max(avg_over_time(scrape_duration_seconds{kubernetes_namespace="{{ .Release.Namespace }}",app="{{ .Release.Name }}-pgmetrics"}[30m])) > {{ .Values.alerts.scrape_duration_threshold }}
      for: 5m
      labels:
        context: database
        support_group: {{ required ".Values.alerts.support_group missing" .Values.alerts.support_group }}
        tier: {{ required ".Values.alerts.tier missing" .Values.alerts.tier }}
        service: {{ $service }}
        severity: info
        meta: Slow metrics collection in postgres-exporter
      annotations:
        summary: Slow metrics collection in postgres-exporter
        description: >
          The {{ .Release.Name }}-pgmetrics pod is taking a long time to gather metrics. Check for throttling in the pgmetrics pod. If that's not a problem, this alert can be an early indication that the PostgreSQL server performance is degrading.

{{- end }}
