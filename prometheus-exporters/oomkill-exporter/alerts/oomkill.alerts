groups:
- name: oomkill.alerts
  rules:
  - alert: PodOOMKilled
    # NOTE: limes-collect-ccloud has a known small-scale memory leak and tends to crash a few times per day. I don't care about this, as long as it's still doing its job. -- @majewsky
    expr: sum(changes(klog_pod_oomkill{pod_name!~"limes-collect-.*"}[24h]) > 0 or ((klog_pod_oomkill == 1) unless (klog_pod_oomkill offset 24h == 1))) by (namespace, pod_name)
    for: 5m
    labels:
      tier: k8s
      service: resources
      severity: info
      context: memory
      meta: "Pod {{ $labels.namespace }}/{{ $labels.pod_name }} OOMKilled"
      playbook: 'docs/support/playbook/kubernetes/k8s_pod_oomkilled'
      no_alert_on_absence: "true" # the underlying metric is only generated after the first oomkill
    annotations:
      summary: Pod was oomkilled recently
      description: The pod {{ $labels.namespace }}/{{ $labels.pod_name }} was hit at least once by the oom killer within 24h

  - alert: PodConstantlyOOMKilled
    expr: sum(changes(klog_pod_oomkill[30m]) ) by (namespace, pod_name) > 2
    for: 5m
    labels:
      tier: k8s
      service: resources
      severity: warning
      context: memory
      meta: "Pod {{ $labels.namespace }}/{{ $labels.pod_name }} constantly OOMKilled"
      playbook: 'docs/support/playbook/kubernetes/k8s_pod_oomkilled'
      no_alert_on_absence: "true" # the underlying metric is only generated after the first oomkill
    annotations:
      summary: Pod was oomkilled more than 2 times in 30 minutes
      description: The pod {{ $labels.namespace }}/{{ $labels.pod_name }} killed several times in short succession. This could be due to wrong resource limits.
