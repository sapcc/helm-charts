# vim: set ft=yaml:

groups:
- name: controlplane.alerts
  rules:

    ### Node Labels ###

    - alert: KubernetesNodeLabelMissingZone
      expr: sum(kube_node_labels{label_zone!~"farm|petting-zoo"}) by (node,label_zone) > 0
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: info
        context: label
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_label_missing.html"
      annotations:
        description: Node {{ $labels.node }} is missing the correct zone label. It is currently set to zone='{{ $labels.label_zone }}' Possible scheduling issues.
        summary: Node {{ $labels.node }} is missing the correct zone label.

    - alert: KubernetesNodeLabelMissingSpecies
      expr: sum(kube_node_labels{node=~"storage.*", label_species!="swift-storage"}) by (node,label_species) > 0 OR sum(kube_node_labels{node=~"network.*", label_species!="network"}) by (node,label_species) > 0 OR sum(kube_node_labels{node=~"master.*", label_species!="master"}) by (node,label_species) > 0
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: info
        context: label
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_label_missing.html"
      annotations:
        description: Node {{ $labels.node }} is missing the correct species label. It is currently set to species='{{ $labels.label_species }}' Possible scheduling issues.
        summary: Node {{ $labels.node }} is missing the correct species label.

    ### Node Taints ###

    - alert: KubernetesNodeTaintMissing
      expr: sum(kube_node_spec_taint{node=~"storage.*", value!~"swift-storage.*"}) by (node,value) > 0 OR sum(kube_node_spec_taint{node=~"network.*", value!~"network|alien"}) by (node,value) > 0
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: info
        context: label
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_taint_missing.html"
      annotations:
        description: Node {{ $labels.node }} is missing the correct taint. It is currently set to value='{{ $labels.value}}' Possible scheduling issues.
        summary: Node {{ $labels.node }} is missing the correct taint label.

    ### Node Bridge ###

    - alert: KubernetesNodeBridgeFilterVLANTagged
      expr: kube_node_status_condition{condition="BridgeFilterVLANTagged", status="true"} == 1
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: info
        context: label
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_node_bridge_filter_iptables.html"
      annotations:
        description: VLAN-tagged ARP/IP traffic is filtered by ARPtables/IPtables on {{ $labels.node }}. Network datapath threatened!
        summary: Bridged VLAN-tagged traffic is filtered by IPtables.

    ### Bonding health ###
  
    - alert: NodeBondDegradedNetwork
      expr: sum(node_bonding_active{master="bond1",node=~"[^storage].*cloud.sap"}) by (master, node) < 2
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: critical 
        context: bond 
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_bond_degraded.html"
      annotations:
        description: Bond {{ $labels.master }} on {{ $labels.node }} is degraded.
        summary: Bond {{ $labels.master }} is degraded. Node network connectivity is not HA. Customer network datapath threatened! Switch failover or ACI upgrade will cause an outage!
    
    - alert: NodeBondDegradedMain
      expr: sum(node_bonding_active{master="bond2",node=~".*cloud.sap"}) by (master, node) < 2
      for: 15m
      labels:
        tier: k8s
        service: node
        severity: warning 
        context: bond 
        meta: "{{ $labels.node }}"
        support_group: containers
        playbook: "docs/support/playbook/kubernetes/k8s_bond_degraded.html"
      annotations:
        description: Bond {{ $labels.master }} on {{ $labels.node }} is degraded. Imminent network outage for this node.
        summary: Bond {{ $labels.master }} is degraded. Node network connectivity is not HA. Switch failover or ACI upgrade will cause an outage!

    ### PVC usage ###

    - alert: KubernetesHighPVCUsagePredicted
      expr: sum((kubelet_volume_stats_available_percent* 100 > 70) and (predict_linear(kubelet_volume_stats_available_percent[1d], 7 * 24 * 3600) * 100 > 90)) by (namespace, persistentvolumeclaim)
      for: 1h
      labels:
        tier: k8s
        service: resources
        severity: warning
        context: storage
        meta: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is set to exceed 90% usage soon"
        support_group: containers
        playbook: '/docs/support/playbook/kubernetes/pvc_usage'
      annotations:
        description: "The PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is predicted to exceed 90% storage consumption in the next 7 days."
        summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is set to exceed 90% usage soon"
